{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing, pipeline, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import feature_selection\n",
    "from itertools import product\n",
    "\n",
    "import osgeo \n",
    "import fiona\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import geocoder\n",
    "from scipy import sparse\n",
    "from shapely.geometry import Point\n",
    "from geopandas.tools import sjoin\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\train.json')\n",
    "\n",
    "test_data = pd.read_json(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\test.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data[\"bathrooms\"].loc[19671] = 1.5 \n",
    "test_data[\"bathrooms\"].loc[22977] = 2.0 \n",
    "test_data[\"bathrooms\"].loc[63719] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "index=list(range(train_data.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_data)\n",
    "b=[np.nan]*len(train_data)\n",
    "c=[np.nan]*len(train_data)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_data['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    test_index=index[int((i*train_data.shape[0])/5):int(((i+1)*train_data.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_data.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_data.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "train_data['manager_level_low']=a\n",
    "train_data['manager_level_medium']=b\n",
    "train_data['manager_level_high']=c\n",
    "\n",
    "\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_data['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "for j in range(train_data.shape[0]):\n",
    "    temp=train_data.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_data['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_data['manager_level_low']=a\n",
    "test_data['manager_level_medium']=b\n",
    "test_data['manager_level_high']=c\n",
    "\n",
    "manager_variable = ['manager_level_low','manager_level_medium','manager_level_high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data['target'] = train_data['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "train_data['low'] = train_data['interest_level'].apply(lambda x: 1 if x=='low' else 0)\n",
    "train_data['medium'] = train_data['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\n",
    "train_data['high'] = train_data['interest_level'].apply(lambda x: 1 if x=='high' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Merge training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data\n",
    "                       ,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "features = full_data[[\"features\"]].apply(\n",
    "    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])\n",
    "n = 5\n",
    "\n",
    "feature_counts = Counter()\n",
    "for feature in features.features:\n",
    "    feature_counts.update(feature)\n",
    "feature = sorted([k for (k,v) in feature_counts.items() if v > n])\n",
    "feature[:10]\n",
    "\n",
    "def clean(s):\n",
    "    x = s.replace(\"-\", \"\")\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.replace(\"twenty four hour\", \"24\")\n",
    "    x = x.replace(\"24/7\", \"24\")\n",
    "    x = x.replace(\"24hr\", \"24\")\n",
    "    x = x.replace(\"24-hour\", \"24\")\n",
    "    x = x.replace(\"24hour\", \"24\")\n",
    "    x = x.replace(\"24 hour\", \"24\")\n",
    "    x = x.replace(\"common\", \"cm\")\n",
    "    x = x.replace(\"concierge\", \"doorman\")\n",
    "    x = x.replace(\"bicycle\", \"bike\")\n",
    "    x = x.replace(\"private\", \"pv\")\n",
    "    x = x.replace(\"deco\", \"dc\")\n",
    "    x = x.replace(\"decorative\", \"dc\")\n",
    "    x = x.replace(\"onsite\", \"os\")\n",
    "    x = x.replace(\"outdoor\", \"od\")\n",
    "    x = x.replace(\"ss appliances\", \"stainless\")\n",
    "    return x\n",
    "\n",
    "def feature_hash(x):\n",
    "    cleaned = clean(x, uniq)\n",
    "    key = cleaned[:4].strip()\n",
    "    return key\n",
    "\n",
    "key2original = defaultdict(list)\n",
    "k = 4\n",
    "for f in feature:\n",
    "    cleaned = clean(f)\n",
    "    key = cleaned[:k].strip()\n",
    "    key2original[key].append(f)\n",
    "\n",
    "def to_tuples():\n",
    "    for f in feature:\n",
    "        key = clean(f)[:k].strip()\n",
    "        yield (f, key2original[key][0])\n",
    "        \n",
    "deduped = list(to_tuples())\n",
    "#df = pd.DataFrame(deduped, columns=[\"original_feature\", \"unique_feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " full_data[\"cfeatures\"] = full_data[[\"features\"]].replace(key2original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data.ix[4620, 'price']  = 1025\n",
    "full_data.ix[12168, 'price']  = 3400\n",
    "full_data.ix[32611, 'price']  = 10000\n",
    "\n",
    "full_data.ix[51229,'latitude'] = 40.831835\n",
    "full_data.ix[51229,'longitude'] = -73.921021\n",
    "full_data.ix[113035, 'latitude'] = 40.869699\n",
    "full_data.ix[113035,'longitude'] = -73.243134\n",
    "full_data.ix[104822, 'latitude'] = 40.752816\n",
    "full_data.ix[104822,'longitude'] = -73.970854\n",
    "full_data.ix[72896, 'latitude'] = 40.772661\n",
    "full_data.ix[72896,'longitude'] = -73.955558\n",
    "full_data.ix[78568, 'latitude'] = 40.763670\n",
    "full_data.ix[78568,'longitude'] = -73.958609\n",
    "full_data.ix[109135, 'latitude'] = 40.778075\n",
    "full_data.ix[109135,'longitude'] = -73.952238\n",
    "full_data.ix[17772, 'latitude'] = 40.748368\n",
    "full_data.ix[17772,'longitude'] = -73.976599\n",
    "full_data.ix[67902, 'latitude'] = 40.805668\n",
    "full_data.ix[67902,'longitude'] = -73.941987\n",
    "full_data.ix[21168, 'latitude'] = 40.731960\n",
    "full_data.ix[21168,'longitude'] = -74.002091\n",
    "full_data.ix[55585, 'latitude'] = 40.737547\n",
    "full_data.ix[55585,'longitude'] = -73.984084\n",
    "full_data.ix[45416, 'latitude'] = 40.754539\n",
    "full_data.ix[45416,'longitude'] = -73.998372\n",
    "full_data.ix[17772, 'latitude'] = 40.747632\n",
    "full_data.ix[17772,'longitude'] = -73.974814\n",
    "full_data.ix[109135, 'latitude'] = 40.775065\n",
    "full_data.ix[109135,'longitude'] = -73.948073\n",
    "full_data.ix[39798, 'latitude' ] = 40.747632\n",
    "full_data.ix[39798,'longitude' ] = -73.974814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "missing_data = pd.read_csv(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\a_few_missing2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data.fillna(0, inplace = True)\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'latitude'] = missing_data.lat.values\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'longitude'] = missing_data.long.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Geo Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "missingCoords = full_data[(full_data.longitude == 0) | (full_data.latitude == 0)]\n",
    "missingGeoms = (missingCoords.street_address + ', New York').apply(geocoder.google)\n",
    "\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'latitude'] = missingGeoms.apply(lambda x: x.lat)\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'longitude'] = missingGeoms.apply(lambda x: x.lng)\n",
    "\n",
    "missing_data = pd.DataFrame({'lat':missingGeoms.apply(lambda x: x.lat), 'long':missingGeoms.apply(lambda x: x.lng)})\n",
    "missing_data.to_csv(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\a_few_missing2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['geometry'] = full_data.apply(lambda x: Point((float(x.longitude), float(x.latitude))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "poly = gpd.GeoDataFrame.from_file(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\nynta.geojson')\n",
    "gdat = gpd.GeoDataFrame(full_data, crs = poly.crs, geometry='geometry')\n",
    "geo_data = sjoin(gdat,poly, how='left', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>building_id</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>display_address</th>\n",
       "      <th>features</th>\n",
       "      <th>high</th>\n",
       "      <th>interest_level</th>\n",
       "      <th>latitude</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>low</th>\n",
       "      <th>manager_id</th>\n",
       "      <th>manager_level_high</th>\n",
       "      <th>manager_level_low</th>\n",
       "      <th>manager_level_medium</th>\n",
       "      <th>medium</th>\n",
       "      <th>photos</th>\n",
       "      <th>price</th>\n",
       "      <th>street_address</th>\n",
       "      <th>target</th>\n",
       "      <th>cfeatures</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>borocode</th>\n",
       "      <th>boroname</th>\n",
       "      <th>cartodb_id</th>\n",
       "      <th>countyfips</th>\n",
       "      <th>created_at</th>\n",
       "      <th>ntacode</th>\n",
       "      <th>ntaname</th>\n",
       "      <th>shape_area</th>\n",
       "      <th>shape_leng</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>53a5b119ba8f7b61d4e010512e0dfc85</td>\n",
       "      <td>2016-06-24 07:54:24</td>\n",
       "      <td>A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...</td>\n",
       "      <td>Metropolitan Avenue</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medium</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>7211212</td>\n",
       "      <td>-73.9425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5ba989232d0489da1b5f2c45f6688adc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[https://photos.renthop.com/2/7211212_1ed4542e...</td>\n",
       "      <td>3000</td>\n",
       "      <td>792 Metropolitan Avenue</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>POINT (-73.9425 40.7145)</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>90.0</td>\n",
       "      <td>047</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "      <td>BK90</td>\n",
       "      <td>East Williamsburg</td>\n",
       "      <td>3.914106e+07</td>\n",
       "      <td>49188.619678</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>c5c8a357cba207596b04d1afd1e4f130</td>\n",
       "      <td>2016-06-12 12:19:27</td>\n",
       "      <td></td>\n",
       "      <td>Columbus Avenue</td>\n",
       "      <td>[Doorman, Elevator, Fitness Center, Cats Allow...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>40.7947</td>\n",
       "      <td>7150865</td>\n",
       "      <td>-73.9667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7533621a882f71e25173b27e3139d83d</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[https://photos.renthop.com/2/7150865_be3306c5...</td>\n",
       "      <td>5465</td>\n",
       "      <td>808 Columbus Avenue</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Doorman, Elevator, Fitness Center, Cats Allow...</td>\n",
       "      <td>POINT (-73.9667 40.7947)</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>124.0</td>\n",
       "      <td>061</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "      <td>MN12</td>\n",
       "      <td>Upper West Side</td>\n",
       "      <td>3.438105e+07</td>\n",
       "      <td>29158.132670</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>c3ba40552e2120b0acfc3cb5730bb2aa</td>\n",
       "      <td>2016-04-17 03:26:41</td>\n",
       "      <td>Top Top West Village location, beautiful Pre-w...</td>\n",
       "      <td>W 13 Street</td>\n",
       "      <td>[Laundry In Building, Dishwasher, Hardwood Flo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>high</td>\n",
       "      <td>40.7388</td>\n",
       "      <td>6887163</td>\n",
       "      <td>-74.0018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>d9039c43983f6e564b1482b273bd7b01</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.603604</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[https://photos.renthop.com/2/6887163_de85c427...</td>\n",
       "      <td>2850</td>\n",
       "      <td>241 W 13 Street</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[Laundry In Building, Dishwasher, Hardwood Flo...</td>\n",
       "      <td>POINT (-74.0018 40.7388)</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>46.0</td>\n",
       "      <td>061</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "      <td>MN23</td>\n",
       "      <td>West Village</td>\n",
       "      <td>2.500053e+07</td>\n",
       "      <td>29385.030305</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28d9ad350afeaab8027513a3e52ac8d5</td>\n",
       "      <td>2016-04-18 02:22:02</td>\n",
       "      <td>Building Amenities - Garage - Garden - fitness...</td>\n",
       "      <td>East 49th Street</td>\n",
       "      <td>[Hardwood Floors, No Fee]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>40.7539</td>\n",
       "      <td>6888711</td>\n",
       "      <td>-73.9677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1067e078446a7897d2da493d2f741316</td>\n",
       "      <td>0.046358</td>\n",
       "      <td>0.827815</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[https://photos.renthop.com/2/6888711_6e660cee...</td>\n",
       "      <td>3275</td>\n",
       "      <td>333 East 49th Street</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Hardwood Floors, No Fee]</td>\n",
       "      <td>POINT (-73.96769999999999 40.7539)</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>109.0</td>\n",
       "      <td>061</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "      <td>MN19</td>\n",
       "      <td>Turtle Bay-East Midtown</td>\n",
       "      <td>1.739787e+07</td>\n",
       "      <td>21638.781206</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-28 01:32:41</td>\n",
       "      <td>Beautifully renovated 3 bedroom flex 4 bedroom...</td>\n",
       "      <td>West 143rd Street</td>\n",
       "      <td>[Pre-War]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>40.8241</td>\n",
       "      <td>6934781</td>\n",
       "      <td>-73.9493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98e13ad4b495b9613cef886d79a6291f</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[https://photos.renthop.com/2/6934781_1fa4b41a...</td>\n",
       "      <td>3350</td>\n",
       "      <td>500 West 143rd Street</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Pre-War]</td>\n",
       "      <td>POINT (-73.94929999999999 40.8241)</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>151.0</td>\n",
       "      <td>061</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "      <td>MN04</td>\n",
       "      <td>Hamilton Heights</td>\n",
       "      <td>1.609379e+07</td>\n",
       "      <td>17410.823299</td>\n",
       "      <td>2015-07-16T02:50:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bathrooms  bedrooms                       building_id  \\\n",
       "10            1.5         3  53a5b119ba8f7b61d4e010512e0dfc85   \n",
       "10000         1.0         2  c5c8a357cba207596b04d1afd1e4f130   \n",
       "100004        1.0         1  c3ba40552e2120b0acfc3cb5730bb2aa   \n",
       "100007        1.0         1  28d9ad350afeaab8027513a3e52ac8d5   \n",
       "100013        1.0         4                                 0   \n",
       "\n",
       "                    created  \\\n",
       "10      2016-06-24 07:54:24   \n",
       "10000   2016-06-12 12:19:27   \n",
       "100004  2016-04-17 03:26:41   \n",
       "100007  2016-04-18 02:22:02   \n",
       "100013  2016-04-28 01:32:41   \n",
       "\n",
       "                                              description  \\\n",
       "10      A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...   \n",
       "10000                                                       \n",
       "100004  Top Top West Village location, beautiful Pre-w...   \n",
       "100007  Building Amenities - Garage - Garden - fitness...   \n",
       "100013  Beautifully renovated 3 bedroom flex 4 bedroom...   \n",
       "\n",
       "            display_address  \\\n",
       "10      Metropolitan Avenue   \n",
       "10000       Columbus Avenue   \n",
       "100004          W 13 Street   \n",
       "100007     East 49th Street   \n",
       "100013    West 143rd Street   \n",
       "\n",
       "                                                 features  high  \\\n",
       "10                                                     []   0.0   \n",
       "10000   [Doorman, Elevator, Fitness Center, Cats Allow...   0.0   \n",
       "100004  [Laundry In Building, Dishwasher, Hardwood Flo...   1.0   \n",
       "100007                          [Hardwood Floors, No Fee]   0.0   \n",
       "100013                                          [Pre-War]   0.0   \n",
       "\n",
       "       interest_level  latitude  listing_id  longitude  low  \\\n",
       "10             medium   40.7145     7211212   -73.9425  0.0   \n",
       "10000             low   40.7947     7150865   -73.9667  1.0   \n",
       "100004           high   40.7388     6887163   -74.0018  0.0   \n",
       "100007            low   40.7539     6888711   -73.9677  1.0   \n",
       "100013            low   40.8241     6934781   -73.9493  1.0   \n",
       "\n",
       "                              manager_id  manager_level_high  \\\n",
       "10      5ba989232d0489da1b5f2c45f6688adc            0.000000   \n",
       "10000   7533621a882f71e25173b27e3139d83d            0.000000   \n",
       "100004  d9039c43983f6e564b1482b273bd7b01            0.045045   \n",
       "100007  1067e078446a7897d2da493d2f741316            0.046358   \n",
       "100013  98e13ad4b495b9613cef886d79a6291f            0.000000   \n",
       "\n",
       "        manager_level_low  manager_level_medium  medium  \\\n",
       "10               0.746479              0.253521     1.0   \n",
       "10000            1.000000              0.000000     0.0   \n",
       "100004           0.603604              0.351351     0.0   \n",
       "100007           0.827815              0.125828     0.0   \n",
       "100013           1.000000              0.000000     0.0   \n",
       "\n",
       "                                                   photos  price  \\\n",
       "10      [https://photos.renthop.com/2/7211212_1ed4542e...   3000   \n",
       "10000   [https://photos.renthop.com/2/7150865_be3306c5...   5465   \n",
       "100004  [https://photos.renthop.com/2/6887163_de85c427...   2850   \n",
       "100007  [https://photos.renthop.com/2/6888711_6e660cee...   3275   \n",
       "100013  [https://photos.renthop.com/2/6934781_1fa4b41a...   3350   \n",
       "\n",
       "                 street_address  target  \\\n",
       "10      792 Metropolitan Avenue     1.0   \n",
       "10000       808 Columbus Avenue     0.0   \n",
       "100004          241 W 13 Street     2.0   \n",
       "100007     333 East 49th Street     0.0   \n",
       "100013    500 West 143rd Street     0.0   \n",
       "\n",
       "                                                cfeatures  \\\n",
       "10                                                     []   \n",
       "10000   [Doorman, Elevator, Fitness Center, Cats Allow...   \n",
       "100004  [Laundry In Building, Dishwasher, Hardwood Flo...   \n",
       "100007                          [Hardwood Floors, No Fee]   \n",
       "100013                                          [Pre-War]   \n",
       "\n",
       "                                  geometry  index_right  borocode   boroname  \\\n",
       "10                POINT (-73.9425 40.7145)         29.0       3.0   Brooklyn   \n",
       "10000             POINT (-73.9667 40.7947)        147.0       1.0  Manhattan   \n",
       "100004            POINT (-74.0018 40.7388)         86.0       1.0  Manhattan   \n",
       "100007  POINT (-73.96769999999999 40.7539)        134.0       1.0  Manhattan   \n",
       "100013  POINT (-73.94929999999999 40.8241)        133.0       1.0  Manhattan   \n",
       "\n",
       "        cartodb_id countyfips           created_at ntacode  \\\n",
       "10            90.0        047  2015-07-16T02:50:22    BK90   \n",
       "10000        124.0        061  2015-07-16T02:50:22    MN12   \n",
       "100004        46.0        061  2015-07-16T02:50:22    MN23   \n",
       "100007       109.0        061  2015-07-16T02:50:22    MN19   \n",
       "100013       151.0        061  2015-07-16T02:50:22    MN04   \n",
       "\n",
       "                        ntaname    shape_area    shape_leng  \\\n",
       "10            East Williamsburg  3.914106e+07  49188.619678   \n",
       "10000           Upper West Side  3.438105e+07  29158.132670   \n",
       "100004             West Village  2.500053e+07  29385.030305   \n",
       "100007  Turtle Bay-East Midtown  1.739787e+07  21638.781206   \n",
       "100013         Hamilton Heights  1.609379e+07  17410.823299   \n",
       "\n",
       "                 updated_at  \n",
       "10      2015-07-16T02:50:22  \n",
       "10000   2015-07-16T02:50:22  \n",
       "100004  2015-07-16T02:50:22  \n",
       "100007  2015-07-16T02:50:22  \n",
       "100013  2015-07-16T02:50:22  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 40)\n",
    "geo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo_var = ['boroname','borocode','ntaname']\n",
    "full_data = pd.merge(left = full_data, right = geo_data[geo_var],left_index=True,right_index=True, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Group Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address', 'borocode','ntanew']\n",
    "text_vars = ['description','features']\n",
    "date_var = 'created'\n",
    "image_var = 'photos'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['ntanew'] = full_data[['boroname', 'ntaname']].apply(lambda x: x[1] if (x[0] == 1) or (x[0] ==3) else x[0], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Date variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full_data['created_year']=full_data['created_datetime'].apply(lambda x:x.year) ## low variant\n",
    "full_data['created_month']=full_data['created_datetime'].apply(lambda x:x.month)\n",
    "full_data['created_day']=full_data['created_datetime'].apply(lambda x:x.day)\n",
    "full_data['created_dayofweek']=full_data['created_datetime'].apply(lambda x:x.dayofweek)\n",
    "full_data['created_dayofyear']=full_data['created_datetime'].apply(lambda x:x.dayofyear)\n",
    "full_data['created_weekofyear']=full_data['created_datetime'].apply(lambda x:x.weekofyear)\n",
    "full_data['created_hour']=full_data['created_datetime'].apply(lambda x:x.hour)\n",
    "full_data['created_epoch']=full_data['created_datetime'].apply(lambda x:x.value//10**9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "date_num_vars = ['created_month','created_dayofweek','created_dayofyear'\n",
    "                 ,'created_weekofyear','created_hour','created_epoch', 'photo_created_hour', 'adv2']#, 'photo_created_dayofyear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['photo_created'] =  pd.to_datetime(full_data['time_stamp'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50       2016-11-09 18:09:22\n",
       "51       2016-11-09 18:09:40\n",
       "52       2016-11-09 18:10:02\n",
       "53       2016-11-09 18:10:08\n",
       "54       2016-11-09 18:10:10\n",
       "55       2016-11-09 18:10:46\n",
       "56       2016-11-09 18:10:52\n",
       "57       2016-11-22 02:54:46\n",
       "58       2016-11-09 18:11:00\n",
       "59       2016-11-02 23:36:42\n",
       "60       2016-11-09 18:11:12\n",
       "61       2016-11-09 18:11:18\n",
       "62       2016-11-09 18:11:34\n",
       "63       2016-11-09 18:11:56\n",
       "64       2016-11-09 18:12:04\n",
       "65       2016-11-22 02:54:54\n",
       "66       2016-11-09 18:12:40\n",
       "67       2016-11-22 02:54:54\n",
       "68       2016-11-09 18:13:18\n",
       "69       2016-11-09 18:13:32\n",
       "70       2016-11-09 18:13:48\n",
       "71       2016-11-09 18:14:02\n",
       "72       2016-11-09 18:14:12\n",
       "73       2016-11-22 02:54:54\n",
       "74       2016-11-09 18:14:24\n",
       "75       2016-11-09 18:14:40\n",
       "76       2016-11-09 18:15:00\n",
       "77       2016-11-09 18:15:06\n",
       "78       2016-11-09 18:15:16\n",
       "79       2016-11-09 18:15:40\n",
       "                 ...        \n",
       "123981   2016-11-09 17:57:06\n",
       "123982   2016-11-09 17:57:10\n",
       "123983   2016-11-09 17:57:14\n",
       "123984   2016-11-02 23:35:52\n",
       "123985   2016-11-09 17:57:26\n",
       "123986   2016-11-09 17:57:26\n",
       "123987   2016-11-09 17:57:38\n",
       "123988   2016-11-09 17:57:42\n",
       "123989   2016-11-09 17:57:44\n",
       "123990   2016-10-28 18:07:10\n",
       "123991   2016-11-09 17:57:50\n",
       "123992   2016-11-09 17:58:08\n",
       "123993   2016-11-09 17:58:10\n",
       "123994   2016-11-09 17:58:16\n",
       "123995   2016-11-09 17:58:20\n",
       "123996   2016-11-09 17:58:22\n",
       "123997   2016-11-09 17:58:24\n",
       "123998   2016-11-09 17:58:28\n",
       "123999   2016-11-09 17:58:30\n",
       "124000   2016-11-02 23:35:54\n",
       "124001   2016-11-09 17:58:36\n",
       "124002   2016-11-09 17:58:40\n",
       "124003   2016-11-09 17:58:48\n",
       "124004   2016-11-22 02:54:26\n",
       "124005   2016-11-22 02:54:26\n",
       "124006   2016-11-22 02:54:26\n",
       "124007   2016-11-22 02:54:26\n",
       "124008   2016-11-09 17:59:30\n",
       "124009   2016-11-09 17:59:32\n",
       "124010   2016-11-09 17:59:34\n",
       "Name: photo_created, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['photo_created'][50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['photo_created_hour']=full_data['photo_created'].apply(lambda x:x.hour)\n",
    "full_data['photo_created_dayofyear']=full_data['photo_created'].apply(lambda x:x.dayofyear)\n",
    "#full_data['photo_created_hour']=full_data['photo_created'].apply(lambda x:x.hour)\n",
    "full_data['adv1']=full_data['time_stamp']/full_data['created_epoch']\n",
    "full_data['adv2']=full_data['photo_created_hour']/full_data['created_hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Additional Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full_data['price']=full_data['price'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \n",
    "full_data['num_of_photos'] = full_data['photos'].apply(lambda x:len(x))\n",
    "full_data['num_of_features'] = full_data['features'].apply(lambda x:len(x))\n",
    "full_data['len_of_desc'] = full_data['description'].apply(lambda x:len(x))\n",
    "full_data['words_of_desc'] = full_data['description'].apply(lambda x:len(re.sub('['+string.punctuation+']', '', x).split()))\n",
    "\n",
    "\n",
    "full_data['nums_of_desc'] = full_data['description']\\\n",
    "        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: len([s for s in x if s.isdigit()]))\n",
    "        \n",
    "full_data['has_phone'] = full_data['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "full_data['has_email'] = full_data['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "full_data['price_latitue'] = (full_data[\"price\"])/ (full_data[\"latitude\"]+1.0) \n",
    "full_data['price_longtitude'] =  (full_data[\"price\"])/ (full_data[\"longitude\"]-1.0)  \n",
    "\n",
    "additional_num_vars = ['rooms','num_of_photos','num_of_features','len_of_desc',\n",
    "                    'words_of_desc','has_phone','has_email','price_latitue', 'price_longtitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Numeric interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['avg_word_len'] = full_data[['len_of_desc','words_of_desc']]\\\n",
    "                                    .apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "    \n",
    "full_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_photo'] = full_data[['price','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "\n",
    "full_data['photos_per_room'] = full_data[['num_of_photos','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "interactive_num_vars = ['avg_word_len','price_per_room','price_per_bedroom','price_per_bathroom','price_per_photo',\n",
    "                        'photos_per_room']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data.borocode.fillna(6, inplace = True)\n",
    "full_data.ntanew.fillna('other', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding building_id\n",
      "Label Encoding manager_id\n",
      "Label Encoding display_address\n",
      "Label Encoding street_address\n",
      "Label Encoding borocode\n",
      "Label Encoding ntanew\n",
      "Label-encoded feaures: ['building_id_le', 'manager_id_le', 'display_address_le', 'street_address_le', 'borocode_le', 'ntanew_le']\n"
     ]
    }
   ],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var])\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (LE_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LE_vars.remove('ntanew_le')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-encoding finished in 0.335424 seconds\n",
      "OHE_sparse size : (124011, 57874)\n",
      "One-hot encoded catgorical feature samples : ['building_0', 'building_00005cb939f9986300d987652c933e15', 'building_00024d77a43f0606f926e2312513845c', 'building_000ae4b7db298401cdae2b0ba1ea8146', 'building_0012f1955391bca600ec301035b97b65', 'building_0021440c04241281a436ec21accc40b1', 'building_002d1eba40aa0a6610e04ff20543585f', 'building_003d8740e21484dcc2280639b25539a4', 'building_00480e54b53fe77d17964be3f8307a99', 'building_00553d95d22484bcc36831c9248d1dbc', 'building_0055c8662ba19e95f78df97592d2b83e', 'building_0056dbdf2881b76f2a0171eb753ec9e0', 'building_0059ae562b9e338a59eaf962cb3eedd2', 'building_005e0f8d7fb7b92be351cbf1dd985149', 'building_0067f166111490e7af7f1a878a67bb5e', 'building_0070bc94a3f80aa717bb15708e98ba54', 'building_0071cda335745940cdae1dc31abfc701', 'building_0078281cd69f4bfec17e42e5cf5eecd9', 'building_0078c2ab46afba9969637ac83621901e', 'building_007ae1cd90420f18bad7b6892a9a1411', 'building_007cd8edc45c6cfbcabd88f70d59a513', 'building_008d3e3a11295305966844713b685f7d', 'building_008ff72d77a8fc85eccfc4ec33ec09a3', 'building_0095cb49c423ec7b204e26d76c56bd35', 'building_009c6ad006e8fd679991c5f8cffaef9f', 'building_009f494b0636f32b96b41926ec7c4bf2', 'building_00a4e18de6c9a7bbec33c77e0588a3b9', 'building_00a61b88186b5115356374b0f5dd0d1e', 'building_00a7b4a6aec7ca1a1635c622918b68f0', 'building_00a94a38fcda000b4448370839a25ac8', 'building_00b2da856a75f0f5690996b0a0b1f397', 'building_00bafd8e05682a7c7e36b4046acd0f1b', 'building_00bb734cde488aa3e1f3e5f1376b9c13', 'building_00cca782a37fc2bb91b080ede56fa7cf', 'building_00ce59a4de554163bda36549de6bf967', 'building_00d1b109f921cd8bc69a203bf35a9bac', 'building_00dfd2bccb9127f2e7966ff29ae1e060', 'building_00e3e6bb0d19bb601842c0ab9589f9a2', 'building_00e8bbc4c74980a06c187165d9a5869e', 'building_00e980b7c97376eb19e0b1be650ccb64', 'building_00ecc203c49a4651cf186de65f308ac1', 'building_00fdda7f129d05ab200c31c0c6de8dfd', 'building_00ffdfd150acc0b097182bbf9dd1db28', 'building_0103d0d57f197a73cdfd0f2f26870d74', 'building_010435ab3b0b415421d583937a55283e', 'building_0106f282d2dfe616303b86e5b68df6b4', 'building_010f3d0141cd76667ca8e3d86e221cf2', 'building_0114c80bf2a9027612083e354d7fbdbc', 'building_0117976b081298aea99e21c327ab25a4', 'building_011a3c781df937c2991a3832b547b158', 'building_01298c8fd1e6e332fb4c188a7183a206', 'building_012fcf2833e0e07897b682ab6f82be3b', 'building_013a96b772f0e46731faee50ad25d727', 'building_01401bc9a8908b2d6ffa84ebf9e1b984', 'building_0145e758b990b8d2648ee57c30762d76', 'building_0152c6255a4e29051b817ce6f3f6dd6f', 'building_0155fa9629c4d68dbe9106b9ac3866ad', 'building_015901f6966b695aec0bda32e36423d6', 'building_015bf4a41140bb7304d338437192f2ab', 'building_015cf622b143eb8703307cf591e9dd46', 'building_01601e509316d5e48176d0f034e04f9f', 'building_016124e48651db465762bfbaf7a9080b', 'building_0162edae269da1472fcbdf4f61df7c7a', 'building_0165775b1775642fb0f71595e83484ba', 'building_0166aa8add8bdc29ca360ab1db5c77f9', 'building_016e30568187a4bd83bf86ca02837544', 'building_017c4715165fe36e5a4372ebf15207f1', 'building_017c9eb62166b050a144adad22b91c00', 'building_017e4ac152c10d537fb2acb3743f2162', 'building_018023115a3d3d3273b25a3357e5358a', 'building_0180389cfd65c7037149466dc934afd3', 'building_01813414c43aedaf8f1b1fb74cef3f99', 'building_018b0a85428622d9a6447fd475cf8bff', 'building_018cd3169a68bd83a92c341d8349b77d', 'building_01904e878eec9c80a40d70f6495454c2', 'building_0195041256b2514a6047a340b0561c3e', 'building_019adbb79eb99ed4c4b9dea91571aa91', 'building_019ce6c2904c70141b3c6fd6d7557344', 'building_019e909a65d7d2e3bf22c6334bf15a7e', 'building_01a430b383a468bcfb6a6dec9f6d42cd', 'building_01a8c78a8ec0234b5e9bd8645fb2ce43', 'building_01acba2c701ccf3528ddf17f1006f694', 'building_01ae28a1834d9bb940cfa2daa78b59c6', 'building_01bb059f7b7619ed7a5f74a97b9ce2ed', 'building_01bd855b1c9b786e37c13c25e5a81fda', 'building_01bfd6631a820706cb16bae93585d998', 'building_01cb79b2d06a9844231c42b3f05eb0c2', 'building_01cb7ce5bc25ca98fcca6ed5236c3224', 'building_01cd1cd0b21bbd22da7381a248d4fdc8', 'building_01cefee671bc17927058c0c579d459cb', 'building_01d767115586534b39d5224710dc56a4', 'building_01d79491cfd8271eaf73a5d69a759ef0', 'building_01d7a001b3a497c25c3a01955d45c339', 'building_01db3503f72909222100231ff6905a78', 'building_01df496b33f2402c5c18076b5f45c916', 'building_01e23bdc82cbdb20a15d290c02572a5d', 'building_01ee5fb703946538f9a34e22f48939e9', 'building_01f27b3de1bf1ee3218e88e64c3315a0', 'building_01f582fe206ca52144a3ac43c14653f7', 'building_01f85713461b5be6a172f4c3db190668']\n"
     ]
    }
   ],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(full_data[LE_vars])\n",
    "OHE_sparse=OHE.transform(full_data[LE_vars])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3. Leave-one-out Encoding\n",
    "\n",
    "Based on the paper \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems\"\n",
    "\n",
    "http://helios.mm.di.uoa.gr/~rouvas/ssi/sigkdd/sigkdd.vol3.1/barreca.ps\n",
    "\n",
    "** A couple of Kaggle scripts: **\n",
    "\n",
    "R version: by Braden Murray: https://www.kaggle.com/brandenkmurray/two-sigma-connect-rental-listing-inquiries/it-is-lit/comments\n",
    "\n",
    "Python Version 1, by Stanislav Ushakov\n",
    "https://www.kaggle.com/stanislavushakov/two-sigma-connect-rental-listing-inquiries/python-version-of-it-is-lit-by-branden/comments\n",
    "\n",
    "Python Version 2, by Rakhlin\n",
    "https://www.kaggle.com/rakhlin/two-sigma-connect-rental-listing-inquiries/another-python-version-of-it-is-lit-by-branden/code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Create a function to encode high-cardinality cateogrical features\n",
    "\n",
    "def designate_single_observations(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n",
    "    \"\"\"\n",
    "    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n",
    "    Classification and Prediction Problems\" by Daniele Micci-Barreca\n",
    "    \"\"\"\n",
    "    hcc_name = \"_\".join([\"hcc\", variable, target])\n",
    "\n",
    "    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n",
    "    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n",
    "    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n",
    "\n",
    "    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n",
    "    if r_k: df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n",
    "\n",
    "    if update_df is None: update_df = test_df\n",
    "    if hcc_name not in update_df.columns: update_df[hcc_name] = np.nan\n",
    "    update_df.update(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for col in ('building_id', 'manager_id', 'display_address'):\n",
    "    train_data, test_data = designate_single_observations(train_data, test_data, col)\n",
    "    \n",
    "prior_low, prior_medium, prior_high = train_data[[\"low\", \"medium\", \"high\"]].mean() \n",
    "\n",
    "skf = model_selection.StratifiedKFold(5)\n",
    "attributes = product((\"building_id\", \"manager_id\"), zip((\"medium\", \"high\"), (prior_medium, prior_high)))\n",
    "for variable, (target, prior) in attributes:\n",
    "    hcc_encode(train_data, test_data, variable, target, prior, k=5, r_k=None)\n",
    "    for train, test in skf.split(np.zeros(len(train_data)), train_data['interest_level']):\n",
    "        hcc_encode(train_data.iloc[train], train_data.iloc[test], variable, target, prior, k=5, r_k=0.01,\n",
    "                   update_df=train_data)\n",
    "        \n",
    "hcc_data = pd.concat([train_data[['building_id', 'manager_id', 'display_address',\n",
    "            'hcc_building_id_medium','hcc_building_id_high',\n",
    "            'hcc_manager_id_medium','hcc_manager_id_high']],\n",
    "           test_data[['building_id', 'manager_id', 'display_address',\n",
    "            'hcc_building_id_medium','hcc_building_id_high',\n",
    "            'hcc_manager_id_medium','hcc_manager_id_high']]\n",
    "           ]\n",
    "          )\n",
    "full_data['building_id'] = hcc_data['building_id']\n",
    "full_data['manager_id'] = hcc_data['manager_id']\n",
    "full_data['display_address'] = hcc_data['display_address']\n",
    "full_data['hcc_building_id_medium'] = hcc_data['hcc_building_id_medium']\n",
    "full_data['hcc_building_id_high'] = hcc_data['hcc_building_id_high']\n",
    "full_data['hcc_manager_id_medium'] = hcc_data['hcc_manager_id_medium']\n",
    "full_data['hcc_manager_id_high'] = hcc_data['hcc_manager_id_high']\n",
    "hcc_vars = ['hcc_building_id_medium','hcc_building_id_high','hcc_manager_id_medium','hcc_manager_id_high']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'building_id', 'created', 'description',\n",
       "       'display_address', 'features', 'high', 'interest_level', 'latitude',\n",
       "       'listing_id', 'longitude', 'low', 'manager_id', 'manager_level_high',\n",
       "       'manager_level_low', 'manager_level_medium', 'medium', 'photos',\n",
       "       'price', 'street_address', 'target', 'cfeatures', 'geometry',\n",
       "       'boroname', 'borocode', 'ntaname', 'ntanew', 'created_datetime',\n",
       "       'created_month', 'created_day', 'created_dayofweek',\n",
       "       'created_dayofyear', 'created_weekofyear', 'created_hour',\n",
       "       'created_epoch', 'rooms', 'num_of_photos', 'num_of_features',\n",
       "       'len_of_desc', 'words_of_desc', 'nums_of_desc', 'has_phone',\n",
       "       'has_email', 'price_latitue', 'price_longtitude', 'avg_word_len',\n",
       "       'price_per_room', 'price_per_bedroom', 'price_per_bathroom',\n",
       "       'price_per_photo', 'photos_per_room', 'building_id_le', 'manager_id_le',\n",
       "       'display_address_le', 'street_address_le', 'borocode_le', 'ntanew_le',\n",
       "       'hcc_building_id_medium', 'hcc_building_id_high',\n",
       "       'hcc_manager_id_medium', 'hcc_manager_id_high'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "scale = preprocessing.StandardScaler()\n",
    "cluster_vars = ['created_weekofyear','latitude', 'longitude', 'bedrooms']\n",
    "cluster_df = full_data[cluster_vars]\n",
    "cluster_df['bedrooms'] = cluster_df['bedrooms'].clip_upper(5)\n",
    "cluster_df['bedrooms'] = cluster_df['bedrooms'].map(lambda x: 0.7 * x)\n",
    "cluster_df['created_weekofyear'] = cluster_df['created_weekofyear'].map(lambda x: 6 * x)\n",
    "cluster_df[cluster_vars] = scale.fit_transform(cluster_df[cluster_vars])\n",
    "db = DBSCAN(eps=0.28, min_samples=25).fit(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_set = set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo = ['latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo = ['latitude', 'longitude']\n",
    "full_data['coords'] = list(zip(full_data.latitude, full_data.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    (40.7145, -73.9425)\n",
       "Name: coords, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['coords'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data[\"cfeatures\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=200)\n",
    "feature_sparse =cntvec.fit_transform(full_data[\"features\"]\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "feature_vars = ['cfeature_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### tf-idf not working, instead, using CountVectorizer, not working either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "# desc_sparse = tfidf.fit_transform(full_data[\"description\"])\n",
    "# desc_vars = ['desc_' + v for v in tfidf.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(stop_words='english', max_features=100)\n",
    "desc_sparse = cntvec.fit_transform(full_data[\"description\"])\n",
    "desc_vars = ['desc_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### word2vec - to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Street Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# embedding\n",
    "\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=10)\n",
    "st_addr_sparse = cntvec.fit_transform(full_data[\"street_address\"])\n",
    "st_addr_vars = ['desc_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'building_id', 'created', 'description',\n",
       "       'display_address', 'features', 'high', 'interest_level', 'latitude',\n",
       "       'listing_id', 'longitude', 'low', 'manager_id', 'manager_level_high',\n",
       "       'manager_level_low', 'manager_level_medium', 'medium', 'photos',\n",
       "       'price', 'street_address', 'target', 'cfeatures', 'geometry',\n",
       "       'boroname', 'borocode', 'ntaname', 'ntanew', 'created_datetime',\n",
       "       'created_month', 'created_day', 'created_dayofweek',\n",
       "       'created_dayofyear', 'created_weekofyear', 'created_hour',\n",
       "       'created_epoch', 'rooms', 'num_of_photos', 'num_of_features',\n",
       "       'len_of_desc', 'words_of_desc', 'nums_of_desc', 'has_phone',\n",
       "       'has_email', 'price_latitue', 'price_longtitude', 'avg_word_len',\n",
       "       'price_per_room', 'price_per_bedroom', 'price_per_bathroom',\n",
       "       'price_per_photo', 'photos_per_room', 'building_id_le', 'manager_id_le',\n",
       "       'display_address_le', 'street_address_le', 'borocode_le', 'ntanew_le',\n",
       "       'hcc_building_id_medium', 'hcc_building_id_high',\n",
       "       'hcc_manager_id_medium', 'hcc_manager_id_high', 'clusters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['boro_bed'] = full_data['borocode'].map(str) + full_data[ 'bedrooms'].clip_upper(4).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### image vars - to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Numberic vs Categorical Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "price_by_manager = full_data.groupby('manager_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_manager.columns = ['manager_id','min_price_by_manager',\n",
    "                            'max_price_by_manager','median_price_by_manager','mean_price_by_manager']\n",
    "full_data = pd.merge(full_data,price_by_manager, how='left',on='manager_id')\n",
    "\n",
    "created_epoch_by_manager = full_data.groupby('manager_id')['created_epoch'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "created_epoch_by_manager.columns = ['manager_id','min_created_epoch_by_manager',\n",
    "                            'max_created_epoch_by_manager','median_created_epoch_by_manager','mean_created_epoch_by_manager']\n",
    "full_data = pd.merge(full_data,created_epoch_by_manager, how='left',on='manager_id')\n",
    "\n",
    "\n",
    "price_by_building = full_data.groupby('building_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','median_price_by_building','mean_price_by_building']\n",
    "full_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n",
    "\n",
    "\n",
    "created_epoch_by_building = full_data.groupby('building_id')['created_epoch'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_created_epoch_by_building',\n",
    "                            'max_created_epoch_by_building','median_created_epoch_by_building','mean_created_epoch_by_building']\n",
    "full_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n",
    "\n",
    "price_by_disp_addr = full_data.groupby('display_address')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_disp_addr.columns = ['display_address','min_price_by_disp_addr',\n",
    "                            'max_price_by_disp_addr','median_price_by_disp_addr','mean_price_by_disp_addr']\n",
    "full_data = pd.merge(full_data,price_by_disp_addr, how='left',on='display_address')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_data['price_percentile_by_manager']=\\\n",
    "            full_data[['price','min_price_by_manager','max_price_by_manager']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "full_data['price_percentile_by_building']=\\\n",
    "            full_data[['price','min_price_by_building','max_price_by_building']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "full_data['price_percentile_by_disp_addr']=\\\n",
    "            full_data[['price','min_price_by_disp_addr','max_price_by_disp_addr']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "\n",
    "\n",
    "full_data['created_epoch_percentile_by_manager']=\\\n",
    "            full_data[['created_epoch','min_created_epoch_by_manager','max_created_epoch_by_manager']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "        \n",
    "price_by_borocode = full_data.groupby('borocode')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_borocode.columns = ['borocode','min_price_by_borocode',\n",
    "                            'max_price_by_borocode','median_price_by_borocode','mean_price_by_borocode']\n",
    "full_data = pd.merge(full_data,price_by_borocode, how='left',on='borocode')\n",
    "\n",
    "full_data['price_percentile_by_borocode']=\\\n",
    "            full_data[['price','min_price_by_borocode','max_price_by_borocode']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "        \n",
    "price_by_boro_bed = full_data.groupby('boro_bed')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_boro_bed.columns = ['boro_bed','min_price_by_boro_bed',\n",
    "                            'max_price_by_boro_bed','median_price_by_boro_bed','mean_price_by_boro_bed']\n",
    "full_data = pd.merge(full_data,price_by_boro_bed, how='left',on='boro_bed')\n",
    "\n",
    "full_data['price_percentile_by_boro_bed']=\\\n",
    "            full_data[['price','min_price_by_boro_bed','max_price_by_boro_bed']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "price_by_cluster = full_data.groupby('clusters')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_cluster.columns = ['clusters','min_price_by_clusters',\n",
    "                            'max_price_by_clusters','median_price_by_clusters','mean_price_by_clusters']\n",
    "full_data = pd.merge(full_data,price_by_cluster, how='left',on='clusters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "price_by_ntanew = full_data.groupby('ntanew')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_ntanew.columns = ['ntanew','min_price_by_ntanew',\n",
    "                            'max_price_by_ntanew','median_price_by_ntanew','mean_price_by_ntanew']\n",
    "full_data = pd.merge(full_data,price_by_ntanew, how='left',on='ntanew')\n",
    "\n",
    "full_data['price_percentile_by_ntanew']=\\\n",
    "            full_data[['price','min_price_by_ntanew','max_price_by_ntanew']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['price_percentile_by_clusters']=\\\n",
    "            full_data[['price','min_price_by_clusters','max_price_by_clusters']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "created_epoch_by_display_address = full_data.groupby('display_address')['created_epoch'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "created_epoch_by_display_address.columns = ['display_address','min_created_epoch_by_display_address',\n",
    "                            'max_created_epoch_by_display_address','median_created_epoch_by_display_address','mean_created_epoch_by_display_address']\n",
    "full_data = pd.merge(full_data,created_epoch_by_display_address, how='left',on='display_address')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "created_dayofyear_by_created_month = full_data.groupby('created_month')['created_dayofyear'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "created_dayofyear_by_created_month.columns = ['created_month','min_created_dayofyear_by_created_month',\n",
    "                            'max_created_dayofyear_by_created_month','median_price_by_created_dayofyear_month','mean_created_dayofyear_by_created_month']\n",
    "full_data = pd.merge(full_data,created_dayofyear_by_created_month, how='left',on='created_month')\n",
    "\n",
    "full_data['created_dayofyear_percentile_by_created_month']=\\\n",
    "            full_data[['created_dayofyear','min_created_dayofyear_by_created_month','max_created_dayofyear_by_created_month']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_cat_vars = ['median_price_by_manager','mean_price_by_manager',\n",
    "                'median_price_by_building','mean_price_by_building',\n",
    "                'median_price_by_disp_addr','mean_price_by_disp_addr',\n",
    "                'median_created_epoch_by_manager','mean_created_epoch_by_manager',\n",
    "                'price_percentile_by_manager','price_percentile_by_building',\n",
    "                'price_percentile_by_disp_addr','created_epoch_percentile_by_manager',\n",
    "                'median_price_by_clusters', 'mean_price_by_clusters', \n",
    "                'price_percentile_by_clusters',\n",
    "                'median_created_epoch_by_display_address',#'mean_created_epoch_by_display_address',\n",
    "                'median_price_by_boro_bed','mean_price_by_boro_bed',\n",
    "                #'median_price_by_ntanew',\n",
    "                'mean_price_by_ntanew','price_percentile_by_ntanew',\n",
    "                'created_dayofyear_percentile_by_created_month'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Listing ID matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_listing_id = full_data['listing_id'].min()\n",
    "max_listing_id = full_data['listing_id'].max()\n",
    "full_data['listing_id_pos']=full_data['listing_id'].apply(lambda x:np.float64((x-min_listing_id+1))/(max_listing_id-min_listing_id+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'listing_id', 'listing_id_pos', 'time_stamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "centroid = (full_data.latitude.sum(axis = 0)/len(full_data)\\\n",
    "    ,full_data.longitude.sum(axis = 0)/len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "geo = ['latitude', 'longitude']\n",
    "full_data['coords'] = list(zip(full_data.latitude, full_data.longitude))\n",
    "def dist_to_center(x):\n",
    "    true_center = np.array((40.7128,-74.0059)).reshape(-1,2)\n",
    "    k = distance.cdist(np.array(x).reshape(-1,2), true_center, 'euclidean') \n",
    "    return k.astype(float)\n",
    "full_data['dist_to_center'] = full_data['coords'].map(dist_to_center)\n",
    "full_data['dist_to_center'] = full_data['dist_to_center'].str[0]\n",
    "full_data['dist_to_center'] = full_data['dist_to_center'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dist_var = ['dist_to_center']#, 'simi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['pos'] = full_data.longitude.round(3).astype(str) + '_' + full_data.latitude.round(3).astype(str)\n",
    "vals = full_data['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "full_data['density'] = full_data['pos'].apply(lambda x: dvals.get(x, vals.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(full_vars)), len(full_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def interactions(dataframe):\n",
    "    combos = list(combinations(list(dataframe.columns), 5))\n",
    "    polynomial = PolynomialFeatures(interaction_only = False, include_bias = False)\n",
    "    dataframe = polynomial.fit_transform(dataframe)\n",
    "    dataframe = pd.DataFrame(dataframe)\n",
    "    noint_indicies = [i for i, x in enumerate(list((dataframe == 0).all())) if x]\n",
    "    dataframe = dataframe.drop(dataframe.columns[noint_indicies], axis = 1)\n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "leak = pd.read_csv(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\listing_image_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "similar = pd.read_csv(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\s3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Listing_Id</th>\n",
       "      <th>time_stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6811957</td>\n",
       "      <td>1479785186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6811958</td>\n",
       "      <td>1479786880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6811960</td>\n",
       "      <td>1479780964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6811964</td>\n",
       "      <td>1479783510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6811965</td>\n",
       "      <td>1479786168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Listing_Id  time_stamp\n",
       "0     6811957  1479785186\n",
       "1     6811958  1479786880\n",
       "2     6811960  1479780964\n",
       "3     6811964  1479783510\n",
       "4     6811965  1479786168"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "similar.columns = ['dz', 'density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dz</th>\n",
       "      <th>simi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7170325</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7092344</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7158677</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7211212</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7225292</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dz      simi\n",
       "0  7170325  1.000000\n",
       "1  7092344  0.692308\n",
       "2  7158677  0.800000\n",
       "3  7211212  0.826087\n",
       "4  7225292  0.800000"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data = pd.merge(full_data, leak, left_on = 'listing_id', right_on = 'Listing_Id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data = pd.merge(full_data, similar, left_on = 'listing_id', right_on = 'dz', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intr = interactions(full_data[['price_per_bedroom', 'dist_to_center']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['int1'] = intr['price_per_bedroom_dist_to_center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms', 'bedrooms',  'price2','latitude', 'longitude', 'listing_id','listing_id_pos', 'time_stamp']# 'listing_id', #'latitude', 'longitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['price2'] = np.log(full_data.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  (49352, 268) testing data size:  (74659, 268)\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[232]\ttrain-mlogloss:0.322181+0.00174294\ttest-mlogloss:0.517714+0.00589766\n",
      "\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "##Baseline with features from \"features\" and street address\n",
    "\n",
    "full_vars = num_vars + date_num_vars\\\n",
    "+ additional_num_vars + interactive_num_vars + LE_vars + hcc_vars + num_cat_vars + manager_variable + dist_var #+ intr_vars\n",
    "#,st_addr_sparse \n",
    "train_x = sparse.hstack([full_data[full_vars],feature_sparse]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "\n",
    "test_x = sparse.hstack([full_data[full_vars], feature_sparse]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "\n",
    "print (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)\n",
    "\n",
    "\n",
    "params = dict()\n",
    "params['objective'] = 'multi:softprob'\n",
    "params['num_class'] = 3\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 6\n",
    "params['min_child_weight'] = 1\n",
    "params['subsample'] = 0.7\n",
    "params['colsample_bytree'] = 0.7\n",
    "params['gamma'] = 1\n",
    "params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "               num_boost_round=1000000, nfold=5,\n",
    "       metrics={'mlogloss'},\n",
    "       seed=1234,\n",
    "       callbacks=[xgb.callback.early_stop(50)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "fs = ExtraTreesClassifier(n_estimators = 50)\n",
    "fs.fit(train_x, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(sorted(zip(full_vars,fs.feature_importances_)\n",
    "                          , key=lambda x: x[1], reverse = False),columns=['feature_name','importance']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = feature_importance[220:].feature_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  (49352, 242) testing data size:  (74659, 242)\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[237]\ttrain-mlogloss:0.375739+0.00157655\ttest-mlogloss:0.552332+0.00624041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_g = sparse.hstack([full_data[d],feature_sparse,st_addr_sparse]).tocsr()[:train_size]\n",
    "train_z = full_data['target'][:train_size].values\n",
    "\n",
    "test_g = sparse.hstack([full_data[d], feature_sparse,st_addr_sparse]).tocsr()[train_size:]\n",
    "test_z = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "print (\"training data size: \", train_g.shape,\"testing data size: \", test_g.shape)\n",
    "\n",
    "\n",
    "params = dict()\n",
    "params['objective'] = 'multi:softprob'\n",
    "params['num_class'] = 3\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 6\n",
    "params['min_child_weight'] = 1\n",
    "params['subsample'] = 0.7\n",
    "params['colsample_bytree'] = 0.7\n",
    "params['gamma'] = 1\n",
    "params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(params, xgb.DMatrix(train_g, label=train_z.reshape(train_x.shape[0],1)),\n",
    "               num_boost_round=1000000, nfold=5,\n",
    "       metrics={'mlogloss'},\n",
    "       seed=1234,\n",
    "       callbacks=[xgb.callback.early_stop(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     feature_name  importance\n",
      "0                            hcc_building_id_high    0.032992\n",
      "1                               price_per_bedroom    0.031587\n",
      "2                          hcc_building_id_medium    0.031213\n",
      "3                             hcc_manager_id_high    0.029496\n",
      "4                   price_percentile_by_disp_addr    0.029403\n",
      "5                                  dist_to_center    0.028653\n",
      "6                                  price_per_room    0.027405\n",
      "7                           hcc_manager_id_medium    0.026968\n",
      "8                     price_percentile_by_manager    0.026531\n",
      "9                                 price_per_photo    0.026156\n",
      "10                              manager_level_low    0.026032\n",
      "11            created_epoch_percentile_by_manager    0.024284\n",
      "12                                   avg_word_len    0.024190\n",
      "13                                 building_id_le    0.023784\n",
      "14                                       latitude    0.023597\n",
      "15                   price_percentile_by_building    0.023160\n",
      "16                             display_address_le    0.023066\n",
      "17                           manager_level_medium    0.023066\n",
      "18                                      longitude    0.022505\n",
      "19                             manager_level_high    0.021849\n",
      "20                              street_address_le    0.021006\n",
      "21  created_dayofyear_percentile_by_created_month    0.020850\n",
      "22                                     listing_id    0.020663\n",
      "23                   price_percentile_by_clusters    0.019071\n",
      "24                               price_longtitude    0.018603\n",
      "25                                  manager_id_le    0.017604\n",
      "26                                    len_of_desc    0.017136\n",
      "27                                  words_of_desc    0.016636\n",
      "28                                        density    0.016106\n",
      "29        median_created_epoch_by_display_address    0.016075\n",
      "..                                            ...         ...\n",
      "32                  mean_created_epoch_by_manager    0.014233\n",
      "33                             price_per_bathroom    0.014108\n",
      "34                median_created_epoch_by_manager    0.013952\n",
      "35                                num_of_features    0.013921\n",
      "36                                   created_hour    0.013827\n",
      "37                        median_price_by_manager    0.013047\n",
      "38                         mean_price_by_building    0.012985\n",
      "39                       median_price_by_building    0.012860\n",
      "40                          mean_price_by_manager    0.011955\n",
      "41                        mean_price_by_disp_addr    0.011674\n",
      "42                                photos_per_room    0.011393\n",
      "43                      median_price_by_disp_addr    0.011299\n",
      "44                              created_dayofweek    0.009458\n",
      "45                                          price    0.007866\n",
      "46                                  created_epoch    0.007772\n",
      "47                                  num_of_photos    0.007585\n",
      "48                                 listing_id_pos    0.005493\n",
      "49                                      bathrooms    0.002684\n",
      "50                                       bedrooms    0.002622\n",
      "51                       median_price_by_clusters    0.002122\n",
      "52                       median_price_by_boro_bed    0.002060\n",
      "53                                          rooms    0.001405\n",
      "54                              created_dayofyear    0.001342\n",
      "55                                    borocode_le    0.001217\n",
      "56                                      has_email    0.001186\n",
      "57                         mean_price_by_clusters    0.001186\n",
      "58                                      has_phone    0.000936\n",
      "59                         mean_price_by_boro_bed    0.000812\n",
      "60                           mean_price_by_ntanew    0.000187\n",
      "61                             created_weekofyear    0.000156\n",
      "\n",
      "[62 rows x 2 columns]\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = xgb.XGBClassifier(learning_rate = 0.1\n",
    "                  , n_estimators =246\n",
    "                  , max_depth = 6\n",
    "                  , min_child_weight = 1\n",
    "                  , subsample = 0.7\n",
    "                  , colsample_bytree = 0.7\n",
    "                  , gamma = 1\n",
    "                  , seed = 1234\n",
    "                  , nthread = -1\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "feature_importance = pd.DataFrame(sorted(zip(full_vars,clf.feature_importances_)\n",
    "                          , key=lambda x: x[1], reverse = True),columns=['feature_name','importance']) \n",
    "\n",
    "print (feature_importance.query('importance>0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_importance.to_csv(r'C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\f.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Manuanl Tuning\n",
    "\n",
    "* Greedy-search\n",
    "* Tune one parameter a time\n",
    "* The results can be used for further tuning (by Bayesian Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1005]\ttrain-mlogloss:0.42341+0.00119897\ttest-mlogloss:0.534216+0.0067717\n",
      "\n",
      "3 0.5342156 1006\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[561]\ttrain-mlogloss:0.403971+0.00151469\ttest-mlogloss:0.533798+0.00632543\n",
      "\n",
      "4 0.5337976 562\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[352]\ttrain-mlogloss:0.385278+0.00205391\ttest-mlogloss:0.534787+0.00661267\n",
      "\n",
      "5 0.5347868 353\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[257]\ttrain-mlogloss:0.354439+0.00180015\ttest-mlogloss:0.534373+0.00687726\n",
      "\n",
      "6 0.5343726 258\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[171]\ttrain-mlogloss:0.338467+0.00183438\ttest-mlogloss:0.536051+0.00662573\n",
      "\n",
      "7 0.5360514 172\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[135]\ttrain-mlogloss:0.303778+0.0032945\ttest-mlogloss:0.537871+0.00653398\n",
      "\n",
      "8 0.5378708 136\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[106]\ttrain-mlogloss:0.27464+0.00294727\ttest-mlogloss:0.540368+0.00668882\n",
      "\n",
      "9 0.5403684 107\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[86]\ttrain-mlogloss:0.243449+0.00486374\ttest-mlogloss:0.544542+0.00590655\n",
      "\n",
      "10 0.5445416 87\n",
      "best max_depth is 4\n",
      "Wall time: 39min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "for max_depth in [3,4,5,6,7,8,9,10]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = 1\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (max_depth,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_max_depth = int(pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['max_depth'].values[0])\n",
    "print ('best max_depth is', best_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[567]\ttrain-mlogloss:0.404098+0.000663067\ttest-mlogloss:0.533047+0.00679302\n",
      "\n",
      "1 0.5330466 568\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[604]\ttrain-mlogloss:0.398721+0.00242664\ttest-mlogloss:0.533035+0.00644301\n",
      "\n",
      "3 0.5330354 605\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[578]\ttrain-mlogloss:0.405064+0.00152694\ttest-mlogloss:0.532609+0.00630762\n",
      "\n",
      "10 0.532609 579\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[591]\ttrain-mlogloss:0.411975+0.00209322\ttest-mlogloss:0.533015+0.00633035\n",
      "\n",
      "30 0.5330148 592\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[701]\ttrain-mlogloss:0.425816+0.00192901\ttest-mlogloss:0.533826+0.00630474\n",
      "\n",
      "100 0.5338258 702\n",
      "best min_child_weight is 10\n",
      "Wall time: 31min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for min_child_weight in [1,3,10,30,100]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (min_child_weight,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_min_child_weight = int(pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['min_child_weight'].values[0])\n",
    "print ('best min_child_weight is', best_min_child_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[711]\ttrain-mlogloss:0.406754+0.00166083\ttest-mlogloss:0.531408+0.0071967\n",
      "\n",
      "0.2 0.5314078 712\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[707]\ttrain-mlogloss:0.399901+0.00127526\ttest-mlogloss:0.531204+0.00676155\n",
      "\n",
      "0.3 0.531204 708\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[631]\ttrain-mlogloss:0.406864+0.00186313\ttest-mlogloss:0.531466+0.00677792\n",
      "\n",
      "0.4 0.5314664 632\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[613]\ttrain-mlogloss:0.406523+0.0014246\ttest-mlogloss:0.531472+0.0066035\n",
      "\n",
      "0.5 0.5314716 614\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[581]\ttrain-mlogloss:0.409756+0.00155808\ttest-mlogloss:0.531746+0.00650111\n",
      "\n",
      "0.6 0.5317456 582\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[513]\ttrain-mlogloss:0.419674+0.00128058\ttest-mlogloss:0.532672+0.00694915\n",
      "\n",
      "0.7 0.5326724 514\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[552]\ttrain-mlogloss:0.411556+0.00148608\ttest-mlogloss:0.532957+0.00609655\n",
      "\n",
      "0.8 0.532957 553\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[650]\ttrain-mlogloss:0.393282+0.0020771\ttest-mlogloss:0.532755+0.00660653\n",
      "\n",
      "0.9 0.5327548 651\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[578]\ttrain-mlogloss:0.405064+0.00152694\ttest-mlogloss:0.532609+0.00630762\n",
      "\n",
      "1 0.532609 579\n",
      "best colsample_bytree is 0.3\n",
      "Wall time: 39min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for colsample_bytree in [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (colsample_bytree,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_colsample_bytree = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['colsample_bytree'].values[0]\n",
    "\n",
    "\n",
    "print ('best colsample_bytree is', best_colsample_bytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[630]\ttrain-mlogloss:0.407024+0.00155329\ttest-mlogloss:0.533097+0.0076172\n",
      "\n",
      "0.6 0.533097 631\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[610]\ttrain-mlogloss:0.409901+0.00195984\ttest-mlogloss:0.531872+0.006592\n",
      "\n",
      "0.7 0.5318718 611\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[734]\ttrain-mlogloss:0.390355+0.00153484\ttest-mlogloss:0.530475+0.00693869\n",
      "\n",
      "0.8 0.5304752 735\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[653]\ttrain-mlogloss:0.403862+0.00110436\ttest-mlogloss:0.530993+0.00710195\n",
      "\n",
      "0.9 0.5309926 654\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[707]\ttrain-mlogloss:0.399901+0.00127526\ttest-mlogloss:0.531204+0.00676155\n",
      "\n",
      "1 0.531204 708\n",
      "best subsample is 0.8\n",
      "Wall time: 17min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for subsample in [0.6,0.7,0.8,0.9,1]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = best_colsample_bytree\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (subsample,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_subsample = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['subsample'].values[0]\n",
    "\n",
    "print ('best subsample is', best_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[734]\ttrain-mlogloss:0.390355+0.00153484\ttest-mlogloss:0.530475+0.00693869\n",
      "\n",
      "0 0.5304752 735\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[727]\ttrain-mlogloss:0.39159+0.00140822\ttest-mlogloss:0.531322+0.00675369\n",
      "\n",
      "0.5 0.5313218 728\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[615]\ttrain-mlogloss:0.409871+0.00171513\ttest-mlogloss:0.531511+0.00670895\n",
      "\n",
      "1 0.5315106 616\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[746]\ttrain-mlogloss:0.39391+0.00154522\ttest-mlogloss:0.530918+0.00699214\n",
      "\n",
      "1.5 0.5309178 747\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[807]\ttrain-mlogloss:0.395925+0.00122783\ttest-mlogloss:0.530799+0.0065077\n",
      "\n",
      "2 0.530799 808\n",
      "best gamma is 0.0\n",
      "Wall time: 19min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for gamma in [0,0.5,1,1.5,2]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['subsample'] = best_subsample\n",
    "    params['colsample_bytree'] = best_colsample_bytree\n",
    "    params['gamma'] = gamma\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (gamma,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_gamma = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['gamma'].values[0]\n",
    "\n",
    "print ('best gamma is', best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Automated Tuning\n",
    "\n",
    "* https://github.com/fmfn/BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[777]\ttrain-mlogloss:0.395416+0.00063035\ttest-mlogloss:0.515122+0.00658389\n",
      "\n",
      "    1 | 05m44s | \u001b[35m  -0.51512\u001b[0m | \u001b[32m            0.3588\u001b[0m | \u001b[32m   1.2582\u001b[0m | \u001b[32m     4.1649\u001b[0m | \u001b[32m           80.6301\u001b[0m | \u001b[32m     0.9264\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[163]\ttrain-mlogloss:0.312694+0.00204997\ttest-mlogloss:0.516923+0.00569277\n",
      "\n",
      "    2 | 03m48s |   -0.51692 |             0.6347 |    1.0328 |      8.0868 |            14.6203 |      0.7682 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[248]\ttrain-mlogloss:0.337428+0.00109834\ttest-mlogloss:0.514474+0.00569808\n",
      "\n",
      "    3 | 04m49s | \u001b[35m  -0.51447\u001b[0m | \u001b[32m            0.5813\u001b[0m | \u001b[32m   0.8058\u001b[0m | \u001b[32m     8.7429\u001b[0m | \u001b[32m           54.3198\u001b[0m | \u001b[32m     0.9855\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1017]\ttrain-mlogloss:0.437482+0.00119263\ttest-mlogloss:0.518355+0.00629745\n",
      "\n",
      "    4 | 04m37s |   -0.51836 |             0.1695 |    0.5426 |      3.3732 |            98.8809 |      0.7892 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1130]\ttrain-mlogloss:0.403606+0.0015737\ttest-mlogloss:0.51586+0.00665169\n",
      "\n",
      "    5 | 06m09s |   -0.51586 |             0.3291 |    1.8168 |      3.3905 |            19.7905 |      0.8893 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[435]\ttrain-mlogloss:0.390504+0.00184784\ttest-mlogloss:0.515415+0.00565413\n",
      "\n",
      "    6 | 05m11s |   -0.51541 |             0.5871 |    1.2286 |      5.3054 |            68.7983 |      0.8560 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1124]\ttrain-mlogloss:0.414823+0.00181639\ttest-mlogloss:0.515974+0.00576688\n",
      "\n",
      "    7 | 05m09s |   -0.51597 |             0.2016 |    0.0088 |      3.1625 |            42.2250 |      0.9754 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[229]\ttrain-mlogloss:0.343603+0.00237595\ttest-mlogloss:0.514398+0.00584858\n",
      "\n",
      "    8 | 03m42s | \u001b[35m  -0.51440\u001b[0m | \u001b[32m            0.4176\u001b[0m | \u001b[32m   1.9934\u001b[0m | \u001b[32m     8.8829\u001b[0m | \u001b[32m           38.2599\u001b[0m | \u001b[32m     0.8823\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[383]\ttrain-mlogloss:0.353322+0.00132279\ttest-mlogloss:0.514425+0.00671818\n",
      "\n",
      "    9 | 04m02s |   -0.51442 |             0.2544 |    1.8926 |      8.9942 |            81.8460 |      0.9695 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1254]\ttrain-mlogloss:0.391512+0.00206262\ttest-mlogloss:0.51542+0.00628958\n",
      "\n",
      "   10 | 06m37s |   -0.51542 |             0.3202 |    1.8639 |      3.3375 |             0.0971 |      0.8995 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[443]\ttrain-mlogloss:0.352024+0.00113657\ttest-mlogloss:0.515664+0.00554929\n",
      "\n",
      "   11 | 03m19s |   -0.51566 |             0.1141 |    1.9992 |      8.7010 |            51.6694 |      0.8786 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[275]\ttrain-mlogloss:0.354009+0.00191779\ttest-mlogloss:0.517102+0.00636717\n",
      "\n",
      "   12 | 05m35s |   -0.51710 |             0.6697 |    0.0138 |      8.9722 |            76.1598 |      0.7411 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[303]\ttrain-mlogloss:0.354609+0.00042264\ttest-mlogloss:0.5158+0.0058321\n",
      "\n",
      "   13 | 07m02s |   -0.51580 |             0.7986 |    1.9437 |      8.1864 |            89.8695 |      0.9989 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[333]\ttrain-mlogloss:0.29842+0.00214611\ttest-mlogloss:0.51676+0.00727174\n",
      "\n",
      "   14 | 02m45s |   -0.51676 |             0.1149 |    0.0690 |      8.6813 |            29.5668 |      0.9888 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1115]\ttrain-mlogloss:0.418239+0.00148579\ttest-mlogloss:0.51679+0.00577817\n",
      "\n",
      "   15 | 09m24s |   -0.51679 |             0.7223 |    1.9795 |      3.4131 |            56.7846 |      0.9768 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[170]\ttrain-mlogloss:0.273972+0.0030398\ttest-mlogloss:0.518575+0.00590322\n",
      "\n",
      "   16 | 04m31s |   -0.51858 |             0.7135 |    1.9493 |      8.9027 |             0.2880 |      0.8531 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[913]\ttrain-mlogloss:0.408546+0.00123379\ttest-mlogloss:0.517804+0.00549316\n",
      "\n",
      "   17 | 07m38s |   -0.51780 |             0.6956 |    0.1067 |      3.1284 |             7.8718 |      0.9880 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[329]\ttrain-mlogloss:0.357984+0.00177086\ttest-mlogloss:0.514458+0.00653716\n",
      "\n",
      "   18 | 03m24s |   -0.51446 |             0.2184 |    1.9817 |      8.9529 |            63.2610 |      0.9445 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1168]\ttrain-mlogloss:0.410731+0.00238413\ttest-mlogloss:0.51679+0.00566056\n",
      "\n",
      "   19 | 09m50s |   -0.51679 |             0.7333 |    1.9693 |      3.5147 |            34.0373 |      0.9829 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[384]\ttrain-mlogloss:0.356878+0.00088527\ttest-mlogloss:0.515479+0.00642993\n",
      "\n",
      "   20 | 04m30s |   -0.51548 |             0.3054 |    1.7675 |      8.8928 |            99.6782 |      0.9649 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[219]\ttrain-mlogloss:0.329947+0.00114714\ttest-mlogloss:0.517056+0.0061736\n",
      "\n",
      "   21 | 05m24s |   -0.51706 |             0.7435 |    0.0435 |      8.7481 |            42.6921 |      0.9856 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[376]\ttrain-mlogloss:0.343622+0.00130585\ttest-mlogloss:0.517753+0.00607094\n",
      "\n",
      "   22 | 02m56s |   -0.51775 |             0.1240 |    0.0452 |      8.6225 |            58.6867 |      0.7312 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[262]\ttrain-mlogloss:0.352768+0.0018726\ttest-mlogloss:0.515662+0.00549865\n",
      "\n",
      "   23 | 06m09s |   -0.51566 |             0.7718 |    1.9804 |      8.9219 |            72.8262 |      0.9284 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[190]\ttrain-mlogloss:0.320059+0.00323888\ttest-mlogloss:0.516885+0.0060685\n",
      "\n",
      "   24 | 05m06s |   -0.51688 |             0.7808 |    1.9159 |      8.4873 |            22.1202 |      0.9783 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1838]\ttrain-mlogloss:0.423701+0.00291289\ttest-mlogloss:0.516552+0.005351\n",
      "\n",
      "   25 | 06m54s |   -0.51655 |             0.1224 |    1.9838 |      3.0512 |            11.7387 |      0.9998 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1726]\ttrain-mlogloss:0.425625+0.0040791\ttest-mlogloss:0.516531+0.00592974\n",
      "\n",
      "   26 | 12m18s |   -0.51653 |             0.5864 |    1.9557 |      3.1101 |            73.1044 |      0.9956 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1206]\ttrain-mlogloss:0.424169+0.00176898\ttest-mlogloss:0.517012+0.00577042\n",
      "\n",
      "   27 | 10m03s |   -0.51701 |             0.7240 |    1.9788 |      3.2914 |            86.3496 |      0.9797 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[396]\ttrain-mlogloss:0.350069+0.00132893\ttest-mlogloss:0.514377+0.00670588\n",
      "\n",
      "   28 | 03m43s | \u001b[35m  -0.51438\u001b[0m | \u001b[32m            0.1997\u001b[0m | \u001b[32m   0.0138\u001b[0m | \u001b[32m     8.9539\u001b[0m | \u001b[32m           97.1450\u001b[0m | \u001b[32m     0.9904\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[455]\ttrain-mlogloss:0.340189+0.000681247\ttest-mlogloss:0.516592+0.00691857\n",
      "\n",
      "   29 | 03m25s |   -0.51659 |             0.1184 |    0.1290 |      8.8683 |            86.0278 |      0.9724 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[276]\ttrain-mlogloss:0.366024+0.000872615\ttest-mlogloss:0.516745+0.00602836\n",
      "\n",
      "   30 | 06m14s |   -0.51674 |             0.7467 |    0.2243 |      8.9576 |            99.8203 |      0.9704 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[536]\ttrain-mlogloss:0.347249+0.00143824\ttest-mlogloss:0.514999+0.00651435\n",
      "\n",
      "   31 | 03m54s |   -0.51500 |             0.1117 |    1.8476 |      8.9536 |            67.9276 |      0.9643 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[532]\ttrain-mlogloss:0.35463+0.00141428\ttest-mlogloss:0.514462+0.00658731\n",
      "\n",
      "   32 | 04m26s |   -0.51446 |             0.1698 |    1.9984 |      8.7388 |            93.8764 |      0.9715 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1571]\ttrain-mlogloss:0.423222+0.00147604\ttest-mlogloss:0.516218+0.00610616\n",
      "\n",
      "   33 | 07m30s |   -0.51622 |             0.2574 |    1.9819 |      3.8146 |            64.5178 |      0.9891 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1008]\ttrain-mlogloss:0.41906+0.00180493\ttest-mlogloss:0.516072+0.00621305\n",
      "\n",
      "   34 | 08m27s |   -0.51607 |             0.7108 |    1.9877 |      3.2263 |            45.3836 |      0.9673 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[230]\ttrain-mlogloss:0.356093+0.00175383\ttest-mlogloss:0.517533+0.005256\n",
      "\n",
      "   35 | 05m31s |   -0.51753 |             0.7713 |    1.9879 |      8.7993 |            55.1292 |      0.7139 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[960]\ttrain-mlogloss:0.417814+0.00183555\ttest-mlogloss:0.516971+0.00634367\n",
      "\n",
      "   36 | 07m31s |   -0.51697 |             0.6302 |    0.0123 |      3.9181 |            50.9725 |      0.9975 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[440]\ttrain-mlogloss:0.344871+0.00131653\ttest-mlogloss:0.515206+0.00613443\n",
      "\n",
      "   37 | 03m09s |   -0.51521 |             0.2086 |    0.0326 |      5.1290 |             0.0866 |      0.9565 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[546]\ttrain-mlogloss:0.314029+0.00221125\ttest-mlogloss:0.515906+0.00793124\n",
      "\n",
      "   38 | 04m03s |   -0.51591 |             0.1085 |    1.8802 |      8.8569 |            34.9928 |      0.9770 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[453]\ttrain-mlogloss:0.332262+0.00256682\ttest-mlogloss:0.515844+0.00715156\n",
      "\n",
      "   39 | 02m53s |   -0.51584 |             0.1067 |    0.0439 |      6.2006 |            17.8388 |      0.9926 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[255]\ttrain-mlogloss:0.345301+0.00167862\ttest-mlogloss:0.515693+0.00567187\n",
      "\n",
      "   40 | 05m26s |   -0.51569 |             0.6599 |    0.2532 |      8.8229 |            65.2168 |      0.9978 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[354]\ttrain-mlogloss:0.283865+0.0015857\ttest-mlogloss:0.516741+0.00681173\n",
      "\n",
      "   41 | 02m55s |   -0.51674 |             0.1065 |    1.6148 |      8.9493 |             7.0755 |      0.9781 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1146]\ttrain-mlogloss:0.41715+0.00189576\ttest-mlogloss:0.517067+0.0064809\n",
      "\n",
      "   42 | 04m30s |   -0.51707 |             0.1239 |    0.0621 |      3.0682 |            25.7448 |      0.9962 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[561]\ttrain-mlogloss:0.363018+0.00161898\ttest-mlogloss:0.514313+0.00674907\n",
      "\n",
      "   43 | 03m39s | \u001b[35m  -0.51431\u001b[0m | \u001b[32m            0.1355\u001b[0m | \u001b[32m   1.9845\u001b[0m | \u001b[32m     6.7880\u001b[0m | \u001b[32m           40.2661\u001b[0m | \u001b[32m     0.9592\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[693]\ttrain-mlogloss:0.380296+0.00165467\ttest-mlogloss:0.514168+0.00637574\n",
      "\n",
      "   44 | 04m13s | \u001b[35m  -0.51417\u001b[0m | \u001b[32m            0.1255\u001b[0m | \u001b[32m   1.9906\u001b[0m | \u001b[32m     6.3741\u001b[0m | \u001b[32m           77.0226\u001b[0m | \u001b[32m     0.9810\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[574]\ttrain-mlogloss:0.362996+0.00125794\ttest-mlogloss:0.5157+0.00682533\n",
      "\n",
      "   45 | 03m31s |   -0.51570 |             0.1105 |    0.2204 |      6.8059 |            72.9255 |      0.9983 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[434]\ttrain-mlogloss:0.348223+0.00207883\ttest-mlogloss:0.514098+0.006327\n",
      "\n",
      "   46 | 04m01s | \u001b[35m  -0.51410\u001b[0m | \u001b[32m            0.1943\u001b[0m | \u001b[32m   1.7898\u001b[0m | \u001b[32m     8.8941\u001b[0m | \u001b[32m           78.5227\u001b[0m | \u001b[32m     0.9908\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[560]\ttrain-mlogloss:0.378597+0.00161878\ttest-mlogloss:0.514456+0.0061784\n",
      "\n",
      "   47 | 04m34s |   -0.51446 |             0.2551 |    1.8769 |      6.5777 |            95.9272 |      0.9970 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[550]\ttrain-mlogloss:0.386273+0.00219269\ttest-mlogloss:0.51574+0.00574227\n",
      "\n",
      "   48 | 07m50s |   -0.51574 |             0.7729 |    1.9750 |      5.9380 |            79.3635 |      0.9981 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1674]\ttrain-mlogloss:0.418354+0.00153762\ttest-mlogloss:0.516475+0.00654\n",
      "\n",
      "   49 | 06m46s |   -0.51648 |             0.1429 |    1.1501 |      3.9225 |            92.2466 |      0.9960 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[330]\ttrain-mlogloss:0.32654+0.00167429\ttest-mlogloss:0.514626+0.00594332\n",
      "\n",
      "   50 | 03m02s |   -0.51463 |             0.1547 |    0.0977 |      8.9525 |            51.9533 |      0.9893 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1118]\ttrain-mlogloss:0.427093+0.00151837\ttest-mlogloss:0.516133+0.00596737\n",
      "\n",
      "   51 | 05m12s |   -0.51613 |             0.2185 |    1.6869 |      3.2948 |            38.3357 |      0.9964 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[405]\ttrain-mlogloss:0.323469+0.00153864\ttest-mlogloss:0.514195+0.00677737\n",
      "\n",
      "   52 | 03m44s |   -0.51419 |             0.1694 |    1.9432 |      8.7366 |            39.1220 |      0.9784 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[322]\ttrain-mlogloss:0.323788+0.00212365\ttest-mlogloss:0.515893+0.00527404\n",
      "\n",
      "   53 | 06m52s |   -0.51589 |             0.7973 |    1.9163 |      7.0146 |            38.3958 |      0.9870 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[439]\ttrain-mlogloss:0.362963+0.00154354\ttest-mlogloss:0.513691+0.00587292\n",
      "\n",
      "   54 | 03m21s | \u001b[35m  -0.51369\u001b[0m | \u001b[32m            0.1816\u001b[0m | \u001b[32m   0.5319\u001b[0m | \u001b[32m     6.8924\u001b[0m | \u001b[32m           54.0081\u001b[0m | \u001b[32m     0.9930\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[619]\ttrain-mlogloss:0.381966+0.00119496\ttest-mlogloss:0.514432+0.00651893\n",
      "\n",
      "   55 | 04m02s |   -0.51443 |             0.1419 |    1.9204 |      6.4850 |            82.8922 |      0.9735 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[439]\ttrain-mlogloss:0.308143+0.00220925\ttest-mlogloss:0.515518+0.00650413\n",
      "\n",
      "   56 | 03m29s |   -0.51552 |             0.1117 |    1.9443 |      8.8417 |            18.3556 |      0.9948 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[431]\ttrain-mlogloss:0.348425+0.00133211\ttest-mlogloss:0.51323+0.00625256\n",
      "\n",
      "   57 | 03m45s | \u001b[35m  -0.51323\u001b[0m | \u001b[32m            0.1910\u001b[0m | \u001b[32m   1.9844\u001b[0m | \u001b[32m     7.5646\u001b[0m | \u001b[32m           45.3687\u001b[0m | \u001b[32m     0.9969\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[536]\ttrain-mlogloss:0.327043+0.00177151\ttest-mlogloss:0.51576+0.00726478\n",
      "\n",
      "   58 | 04m00s |   -0.51576 |             0.1055 |    1.8891 |      8.9233 |            43.6941 |      0.9382 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[529]\ttrain-mlogloss:0.344099+0.00149044\ttest-mlogloss:0.514718+0.00654544\n",
      "\n",
      "   59 | 03m34s |   -0.51472 |             0.1368 |    0.6868 |      6.1552 |            46.3090 |      0.9905 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[533]\ttrain-mlogloss:0.380172+0.00163696\ttest-mlogloss:0.514427+0.00625354\n",
      "\n",
      "   60 | 04m05s |   -0.51443 |             0.2664 |    1.8336 |      5.7029 |            43.2460 |      0.9767 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[439]\ttrain-mlogloss:0.353211+0.00122261\ttest-mlogloss:0.513717+0.00618784\n",
      "\n",
      "   61 | 03m41s |   -0.51372 |             0.1711 |    1.9711 |      7.1689 |            48.5166 |      0.9986 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[561]\ttrain-mlogloss:0.325497+0.00148143\ttest-mlogloss:0.515557+0.00695239\n",
      "\n",
      "   62 | 04m13s |   -0.51556 |             0.1112 |    1.8781 |      8.6866 |            46.8365 |      0.9872 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1366]\ttrain-mlogloss:0.415152+0.0024489\ttest-mlogloss:0.516618+0.00602902\n",
      "\n",
      "   63 | 05m13s |   -0.51662 |             0.1124 |    1.8496 |      3.0743 |             3.0353 |      0.9549 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[547]\ttrain-mlogloss:0.371805+0.00183858\ttest-mlogloss:0.514016+0.00686539\n",
      "\n",
      "   64 | 03m36s |   -0.51402 |             0.1282 |    1.9920 |      6.8883 |            45.8994 |      0.9364 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[640]\ttrain-mlogloss:0.349539+0.00263782\ttest-mlogloss:0.515033+0.00700233\n",
      "\n",
      "   65 | 03m52s |   -0.51503 |             0.1008 |    1.8167 |      6.2390 |            27.3591 |      0.9907 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[796]\ttrain-mlogloss:0.419176+0.00176457\ttest-mlogloss:0.51665+0.00608238\n",
      "\n",
      "   66 | 05m09s |   -0.51665 |             0.4179 |    0.0404 |      3.1892 |             0.1047 |      0.9505 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[261]\ttrain-mlogloss:0.275256+0.00090599\ttest-mlogloss:0.517334+0.00737202\n",
      "\n",
      "   67 | 02m27s |   -0.51733 |             0.1150 |    0.1489 |      8.4604 |            10.4767 |      0.9992 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[605]\ttrain-mlogloss:0.368403+0.00147667\ttest-mlogloss:0.513463+0.00668945\n",
      "\n",
      "   68 | 04m18s |   -0.51346 |             0.1707 |    1.9176 |      6.2413 |            65.8616 |      0.9913 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[555]\ttrain-mlogloss:0.366094+0.00130032\ttest-mlogloss:0.515354+0.00668318\n",
      "\n",
      "   69 | 03m51s |   -0.51535 |             0.1067 |    1.9500 |      7.2693 |            61.6910 |      0.9915 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[498]\ttrain-mlogloss:0.376496+0.00141732\ttest-mlogloss:0.515302+0.0055388\n",
      "\n",
      "   70 | 06m24s |   -0.51530 |             0.6406 |    1.9837 |      5.8059 |            50.9191 |      0.9957 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[604]\ttrain-mlogloss:0.361586+0.00270099\ttest-mlogloss:0.514115+0.00645363\n",
      "\n",
      "   71 | 04m01s |   -0.51412 |             0.1879 |    1.9212 |      5.9264 |            16.3159 |      0.9769 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1467]\ttrain-mlogloss:0.430028+0.00199839\ttest-mlogloss:0.516318+0.0060083\n",
      "\n",
      "   72 | 06m14s |   -0.51632 |             0.1760 |    1.7917 |      3.1893 |            78.5622 |      0.9871 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[705]\ttrain-mlogloss:0.386113+0.00144769\ttest-mlogloss:0.516042+0.00740269\n",
      "\n",
      "   73 | 03m49s |   -0.51604 |             0.1052 |    0.0275 |      5.5218 |            80.4647 |      0.9772 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1331]\ttrain-mlogloss:0.422063+0.00213508\ttest-mlogloss:0.516219+0.00621264\n",
      "\n",
      "   74 | 07m05s |   -0.51622 |             0.3078 |    1.9248 |      3.3468 |            29.5553 |      0.9961 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[405]\ttrain-mlogloss:0.340712+0.00175697\ttest-mlogloss:0.514365+0.00625979\n",
      "\n",
      "   75 | 05m15s |   -0.51437 |             0.3012 |    1.9969 |      8.9755 |            74.9618 |      0.9734 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[700]\ttrain-mlogloss:0.38088+0.000841033\ttest-mlogloss:0.513844+0.00609852\n",
      "\n",
      "   76 | 05m30s |   -0.51384 |             0.2963 |    1.9725 |      5.7135 |            70.0042 |      0.9915 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[464]\ttrain-mlogloss:0.345942+0.00114332\ttest-mlogloss:0.516874+0.00716842\n",
      "\n",
      "   77 | 03m35s |   -0.51687 |             0.1066 |    0.0528 |      8.6436 |            91.8570 |      0.9653 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[384]\ttrain-mlogloss:0.357355+0.00132596\ttest-mlogloss:0.514544+0.00634009\n",
      "\n",
      "   78 | 05m03s |   -0.51454 |             0.3448 |    1.8785 |      8.7975 |            97.1907 |      0.9883 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[571]\ttrain-mlogloss:0.374563+0.00135368\ttest-mlogloss:0.514564+0.00653329\n",
      "\n",
      "   79 | 04m10s |   -0.51456 |             0.1285 |    1.9899 |      7.9716 |            87.7483 |      0.9805 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[374]\ttrain-mlogloss:0.339553+0.00112988\ttest-mlogloss:0.516628+0.00613404\n",
      "\n",
      "   80 | 07m26s |   -0.51663 |             0.7438 |    1.9149 |      7.5922 |            65.9095 |      0.9842 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1860]\ttrain-mlogloss:0.42237+0.00132596\ttest-mlogloss:0.516228+0.00572\n",
      "\n",
      "   81 | 07m08s |   -0.51623 |             0.1168 |    1.6784 |      3.0371 |            68.9732 |      0.9894 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[709]\ttrain-mlogloss:0.373048+0.00162682\ttest-mlogloss:0.515524+0.00673581\n",
      "\n",
      "   82 | 04m08s |   -0.51552 |             0.1293 |    0.0576 |      5.9093 |            66.9003 |      0.9978 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[723]\ttrain-mlogloss:0.39713+0.00151549\ttest-mlogloss:0.51516+0.00643818\n",
      "\n",
      "   83 | 05m04s |   -0.51516 |             0.3106 |    1.9985 |      4.7286 |            23.4883 |      0.9997 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[728]\ttrain-mlogloss:0.357772+0.00200542\ttest-mlogloss:0.51475+0.00720652\n",
      "\n",
      "   84 | 04m08s |   -0.51475 |             0.1153 |    1.9252 |      5.9658 |             9.2068 |      0.9662 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[970]\ttrain-mlogloss:0.388872+0.00189231\ttest-mlogloss:0.51441+0.00639316\n",
      "\n",
      "   85 | 05m12s |   -0.51441 |             0.1148 |    1.9633 |      5.4455 |            73.5470 |      0.9894 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[285]\ttrain-mlogloss:0.318564+0.00172768\ttest-mlogloss:0.51413+0.00717858\n",
      "\n",
      "   86 | 03m08s |   -0.51413 |             0.1817 |    0.1632 |      8.6729 |            38.1169 |      0.9824 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2400]\ttrain-mlogloss:0.41865+0.00147589\ttest-mlogloss:0.515751+0.00577193\n",
      "\n",
      "   87 | 09m00s |   -0.51575 |             0.1124 |    1.8917 |      3.7828 |            52.5951 |      0.9946 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1887]\ttrain-mlogloss:0.43105+0.00179635\ttest-mlogloss:0.51571+0.00607368\n",
      "\n",
      "   88 | 07m42s |   -0.51571 |             0.1513 |    1.8750 |      3.4322 |            83.4762 |      0.9942 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[414]\ttrain-mlogloss:0.354811+0.0023473\ttest-mlogloss:0.516633+0.00589048\n",
      "\n",
      "   89 | 06m25s |   -0.51663 |             0.7485 |    1.8892 |      5.2085 |             0.8730 |      0.9957 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[774]\ttrain-mlogloss:0.383225+0.00145168\ttest-mlogloss:0.515407+0.00702808\n",
      "\n",
      "   90 | 04m42s |   -0.51541 |             0.1563 |    1.9977 |      5.9979 |            68.1409 |      0.7636 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[762]\ttrain-mlogloss:0.363468+0.00156036\ttest-mlogloss:0.515015+0.00667589\n",
      "\n",
      "   91 | 04m42s |   -0.51502 |             0.1066 |    1.9170 |      6.3681 |            55.7290 |      0.9794 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[515]\ttrain-mlogloss:0.30892+0.0014667\ttest-mlogloss:0.514532+0.0069537\n",
      "\n",
      "   92 | 04m03s |   -0.51453 |             0.1245 |    1.9803 |      7.6461 |            12.6608 |      0.9301 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[395]\ttrain-mlogloss:0.331821+0.00123326\ttest-mlogloss:0.515917+0.00626654\n",
      "\n",
      "   93 | 03m31s |   -0.51592 |             0.1176 |    0.7310 |      8.9469 |            62.8348 |      0.9990 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[674]\ttrain-mlogloss:0.405019+0.00232955\ttest-mlogloss:0.516048+0.0061137\n",
      "\n",
      "   94 | 03m39s |   -0.51605 |             0.1301 |    0.0516 |      4.7983 |            32.4645 |      0.9730 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[547]\ttrain-mlogloss:0.343747+0.00199867\ttest-mlogloss:0.514761+0.00631968\n",
      "\n",
      "   95 | 03m56s |   -0.51476 |             0.1320 |    1.9315 |      6.0951 |            20.7008 |      0.9213 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[657]\ttrain-mlogloss:0.365994+0.00128147\ttest-mlogloss:0.516376+0.00646216\n",
      "\n",
      "   96 | 04m14s |   -0.51638 |             0.1112 |    0.0097 |      6.6066 |            98.4301 |      0.9932 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[751]\ttrain-mlogloss:0.361792+0.00169356\ttest-mlogloss:0.515282+0.00628873\n",
      "\n",
      "   97 | 04m42s |   -0.51528 |             0.1035 |    1.6920 |      6.4753 |            63.9817 |      0.9795 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[970]\ttrain-mlogloss:0.382218+0.00155542\ttest-mlogloss:0.514583+0.00665291\n",
      "\n",
      "   98 | 05m31s |   -0.51458 |             0.1305 |    1.9282 |      5.5572 |            66.0953 |      0.9948 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[445]\ttrain-mlogloss:0.327392+0.000973682\ttest-mlogloss:0.515124+0.00668953\n",
      "\n",
      "   99 | 03m49s |   -0.51512 |             0.1179 |    0.0836 |      8.6750 |            70.0615 |      0.9610 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[768]\ttrain-mlogloss:0.347465+0.00238186\ttest-mlogloss:0.515376+0.00693387\n",
      "\n",
      "  100 | 04m27s |   -0.51538 |             0.1055 |    1.5166 |      5.1517 |            14.1212 |      0.9955 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1828]\ttrain-mlogloss:0.420858+0.00137258\ttest-mlogloss:0.516549+0.00630808\n",
      "\n",
      "  101 | 07m20s |   -0.51655 |             0.1172 |    1.9214 |      3.1685 |            59.8934 |      0.9640 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[777]\ttrain-mlogloss:0.377411+0.00183957\ttest-mlogloss:0.513871+0.00657226\n",
      "\n",
      "  102 | 04m58s |   -0.51387 |             0.1465 |    1.8769 |      5.8734 |            48.8277 |      0.9981 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[381]\ttrain-mlogloss:0.321646+0.00211113\ttest-mlogloss:0.515331+0.00672754\n",
      "\n",
      "  103 | 03m37s |   -0.51533 |             0.1410 |    0.1248 |      8.7222 |            55.2909 |      0.9337 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[333]\ttrain-mlogloss:0.351487+0.00204734\ttest-mlogloss:0.513808+0.00558274\n",
      "\n",
      "  104 | 03m32s |   -0.51381 |             0.1887 |    0.3047 |      7.0418 |            51.0699 |      0.9946 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1144]\ttrain-mlogloss:0.427826+0.00130264\ttest-mlogloss:0.516538+0.00604426\n",
      "\n",
      "  105 | 07m47s |   -0.51654 |             0.4715 |    1.8314 |      3.0426 |            95.2458 |      0.9667 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[368]\ttrain-mlogloss:0.347118+0.00127865\ttest-mlogloss:0.514731+0.00659424\n",
      "\n",
      "  106 | 03m31s |   -0.51473 |             0.1636 |    0.3265 |      7.6318 |            53.0555 |      0.9797 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[809]\ttrain-mlogloss:0.396961+0.00152819\ttest-mlogloss:0.515853+0.00663595\n",
      "\n",
      "  107 | 04m27s |   -0.51585 |             0.1472 |    0.0671 |      4.2282 |            55.8355 |      0.9697 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[301]\ttrain-mlogloss:0.345187+0.00145086\ttest-mlogloss:0.513761+0.00573659\n",
      "\n",
      "  108 | 04m32s |   -0.51376 |             0.4010 |    0.0141 |      7.6895 |            49.8492 |      0.9993 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[452]\ttrain-mlogloss:0.299521+0.00246315\ttest-mlogloss:0.516617+0.0069553\n",
      "\n",
      "  109 | 03m35s |   -0.51662 |             0.1015 |    0.2612 |      7.8747 |            24.3847 |      0.9514 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[647]\ttrain-mlogloss:0.350472+0.00187305\ttest-mlogloss:0.515793+0.00623678\n",
      "\n",
      "  110 | 04m05s |   -0.51579 |             0.1208 |    1.9115 |      5.6988 |             0.0528 |      0.8264 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[393]\ttrain-mlogloss:0.358951+0.00159487\ttest-mlogloss:0.516993+0.00585404\n",
      "\n",
      "  111 | 04m44s |   -0.51699 |             0.2671 |    1.9798 |      8.8503 |            84.6420 |      0.7324 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[829]\ttrain-mlogloss:0.384305+0.00159249\ttest-mlogloss:0.514802+0.00610201\n",
      "\n",
      "  112 | 05m45s |   -0.51480 |             0.2071 |    1.9962 |      5.5715 |            89.8543 |      0.9673 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[341]\ttrain-mlogloss:0.299428+0.00150752\ttest-mlogloss:0.515332+0.00665253\n",
      "\n",
      "  113 | 03m50s |   -0.51533 |             0.1627 |    1.9264 |      8.5358 |            14.7865 |      0.9795 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[695]\ttrain-mlogloss:0.372905+0.00194703\ttest-mlogloss:0.514922+0.00687722\n",
      "\n",
      "  114 | 04m36s |   -0.51492 |             0.1069 |    1.7419 |      6.6976 |            70.6318 |      0.9877 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[356]\ttrain-mlogloss:0.323358+0.00114289\ttest-mlogloss:0.515655+0.00704285\n",
      "\n",
      "  115 | 03m12s |   -0.51566 |             0.1444 |    0.0531 |      6.3730 |             4.8267 |      0.9678 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[343]\ttrain-mlogloss:0.337194+0.00296907\ttest-mlogloss:0.514661+0.00511772\n",
      "\n",
      "  116 | 05m58s |   -0.51466 |             0.6290 |    1.8793 |      6.1788 |            18.4199 |      0.9869 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[503]\ttrain-mlogloss:0.347029+0.00222217\ttest-mlogloss:0.515798+0.00647908\n",
      "\n",
      "  117 | 03m47s |   -0.51580 |             0.1064 |    0.0208 |      6.4468 |            38.9937 |      0.9999 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[574]\ttrain-mlogloss:0.32686+0.00193151\ttest-mlogloss:0.515155+0.00654801\n",
      "\n",
      "  118 | 04m23s |   -0.51516 |             0.1047 |    1.4310 |      7.1758 |            44.8643 |      0.9889 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[298]\ttrain-mlogloss:0.342663+0.00198113\ttest-mlogloss:0.515721+0.00586914\n",
      "\n",
      "  119 | 06m52s |   -0.51572 |             0.7685 |    1.9733 |      7.1126 |            46.0491 |      0.9883 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1148]\ttrain-mlogloss:0.39402+0.00163368\ttest-mlogloss:0.514535+0.00592981\n",
      "\n",
      "  120 | 05m51s |   -0.51453 |             0.1303 |    1.9257 |      4.1552 |            42.4178 |      0.9927 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1045]\ttrain-mlogloss:0.404783+0.00174323\ttest-mlogloss:0.515459+0.00653008\n",
      "\n",
      "  121 | 06m03s |   -0.51546 |             0.1911 |    1.8380 |      4.8591 |            99.9362 |      0.9565 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1119]\ttrain-mlogloss:0.426885+0.00126742\ttest-mlogloss:0.517198+0.00586951\n",
      "\n",
      "  122 | 04m56s |   -0.51720 |             0.1115 |    0.4604 |      3.0917 |            46.9713 |      0.9929 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1001]\ttrain-mlogloss:0.377423+0.00191983\ttest-mlogloss:0.515482+0.00662869\n",
      "\n",
      "  123 | 05m14s |   -0.51548 |             0.1145 |    1.9054 |      4.8525 |             6.4949 |      0.9261 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[455]\ttrain-mlogloss:0.366849+0.00144101\ttest-mlogloss:0.515328+0.00655806\n",
      "\n",
      "  124 | 03m34s |   -0.51533 |             0.1158 |    0.3268 |      6.3924 |            49.3428 |      0.9580 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[549]\ttrain-mlogloss:0.35574+0.00296541\ttest-mlogloss:0.513777+0.00663797\n",
      "\n",
      "  125 | 04m25s |   -0.51378 |             0.1439 |    1.9638 |      6.3575 |            31.6885 |      0.9990 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[247]\ttrain-mlogloss:0.340207+0.00229534\ttest-mlogloss:0.515911+0.00519674\n",
      "\n",
      "  126 | 05m55s |   -0.51591 |             0.7289 |    1.9695 |      7.1735 |            29.7798 |      0.9664 | \n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1))\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = int(max_depth )   \n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['verbose_eval'] = True    \n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain,\n",
    "                       num_boost_round=100000,\n",
    "                       nfold=5,\n",
    "                       metrics={'mlogloss'},\n",
    "                       seed=1234,\n",
    "                       callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mlogloss-mean'].min()\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'max_depth': (3, 9),\n",
    "                              'min_child_weight': (0, 100),\n",
    "                              'colsample_bytree': (0.1, 0.8),\n",
    "                              'subsample': (0.7, 1),\n",
    "                              'gamma': (0, 2)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "xgb_BO.maximize(init_points=6, n_iter=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.190996</td>\n",
       "      <td>1.984392</td>\n",
       "      <td>7.564569</td>\n",
       "      <td>45.368728</td>\n",
       "      <td>0.996902</td>\n",
       "      <td>-0.513230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.170692</td>\n",
       "      <td>1.917625</td>\n",
       "      <td>6.241328</td>\n",
       "      <td>65.861638</td>\n",
       "      <td>0.991324</td>\n",
       "      <td>-0.513463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.181628</td>\n",
       "      <td>0.531858</td>\n",
       "      <td>6.892402</td>\n",
       "      <td>54.008102</td>\n",
       "      <td>0.992965</td>\n",
       "      <td>-0.513691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.171070</td>\n",
       "      <td>1.971115</td>\n",
       "      <td>7.168901</td>\n",
       "      <td>48.516582</td>\n",
       "      <td>0.998621</td>\n",
       "      <td>-0.513717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.401026</td>\n",
       "      <td>0.014076</td>\n",
       "      <td>7.689483</td>\n",
       "      <td>49.849235</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>-0.513761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.143883</td>\n",
       "      <td>1.963752</td>\n",
       "      <td>6.357500</td>\n",
       "      <td>31.688469</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>-0.513777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.188695</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>7.041766</td>\n",
       "      <td>51.069938</td>\n",
       "      <td>0.994610</td>\n",
       "      <td>-0.513808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.296286</td>\n",
       "      <td>1.972459</td>\n",
       "      <td>5.713547</td>\n",
       "      <td>70.004185</td>\n",
       "      <td>0.991461</td>\n",
       "      <td>-0.513844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     colsample_bytree     gamma  max_depth  min_child_weight  subsample  \\\n",
       "50           0.190996  1.984392   7.564569         45.368728   0.996902   \n",
       "61           0.170692  1.917625   6.241328         65.861638   0.991324   \n",
       "47           0.181628  0.531858   6.892402         54.008102   0.992965   \n",
       "54           0.171070  1.971115   7.168901         48.516582   0.998621   \n",
       "101          0.401026  0.014076   7.689483         49.849235   0.999282   \n",
       "118          0.143883  1.963752   6.357500         31.688469   0.999016   \n",
       "97           0.188695  0.304747   7.041766         51.069938   0.994610   \n",
       "69           0.296286  1.972459   5.713547         70.004185   0.991461   \n",
       "\n",
       "        score  \n",
       "50  -0.513230  \n",
       "61  -0.513463  \n",
       "47  -0.513691  \n",
       "54  -0.513717  \n",
       "101 -0.513761  \n",
       "118 -0.513777  \n",
       "97  -0.513808  \n",
       "69  -0.513844  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show tuning results\n",
    "BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "BO_scores = BO_scores.sort_values(by='score',ascending=False)\n",
    "\n",
    "BO_scores.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BO_scores.loc[-1] = [ 0.4699,1.9776, 6.7747,19.4144,0.7892, -0.52662 ]  # adding a row\n",
    "BO_scores.index = BO_scores.index + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BO_scores = BO_scores.sort_values(by='score',ascending=False)\n",
    "\n",
    "BO_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Re-train models\n",
    "\n",
    "Now we have optimized parameters, let's decrease the size of learning rate and train the model for better results.\n",
    "\n",
    "Firstly we'll use xgb.cv again to get optimal n_estimators, then we can use tuned n_esimator to finally train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 500 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4665]\ttrain-mlogloss:0.333136+0.0012015\ttest-mlogloss:0.521276+0.00964687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "params['objective'] = 'multi:softprob'\n",
    "params['num_class'] = 3\n",
    "params['eta'] = 0.01\n",
    "params['max_depth'] = int(BO_scores.to_dict()['max_depth'][0])\n",
    "params['min_child_weight'] = BO_scores.to_dict()['min_child_weight'][0]\n",
    "params['colsample_bytree'] = BO_scores.to_dict()['colsample_bytree'][0]\n",
    "params['subsample'] = BO_scores.to_dict()['subsample'][0]\n",
    "params['gamma'] = BO_scores.to_dict()['gamma'][0]\n",
    "params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "               num_boost_round=1000000, nfold=10,\n",
    "       metrics={'mlogloss'},\n",
    "       seed=1234,\n",
    "       callbacks=[xgb.callback.early_stop(500)])\n",
    "\n",
    "best_iteration = len(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                feature_name  importance\n",
      "0                       hcc_building_id_high    0.031839\n",
      "1                     hcc_building_id_medium    0.031707\n",
      "2                          price_per_bedroom    0.030547\n",
      "3                                      price    0.030518\n",
      "4                        hcc_manager_id_high    0.030303\n",
      "5                price_percentile_by_manager    0.030082\n",
      "6                             price_per_room    0.028758\n",
      "7                             dist_to_center    0.028652\n",
      "8              price_percentile_by_disp_addr    0.027419\n",
      "9                       manager_level_medium    0.026829\n",
      "10                           price_per_photo    0.025932\n",
      "11                              avg_word_len    0.025616\n",
      "12       created_epoch_percentile_by_manager    0.025510\n",
      "13                     hcc_manager_id_medium    0.025508\n",
      "14                                  latitude    0.025462\n",
      "15              price_percentile_by_building    0.025379\n",
      "16                         manager_level_low    0.025069\n",
      "17                            building_id_le    0.024610\n",
      "18                        manager_level_high    0.024129\n",
      "19                                 longitude    0.022971\n",
      "20                        display_address_le    0.022802\n",
      "21                         street_address_le    0.022008\n",
      "22                                listing_id    0.019695\n",
      "23                               len_of_desc    0.018898\n",
      "24                             manager_id_le    0.018746\n",
      "25                        price_per_bathroom    0.018359\n",
      "26                             words_of_desc    0.018290\n",
      "27             mean_created_epoch_by_manager    0.015412\n",
      "28           median_created_epoch_by_manager    0.014911\n",
      "29   median_created_epoch_by_display_address    0.014905\n",
      "..                                       ...         ...\n",
      "104                    feature_live_in_super    0.000158\n",
      "105                                desc_34th    0.000155\n",
      "106                   feature_video_intercom    0.000129\n",
      "107                            created_month    0.000123\n",
      "108                  feature_luxury_building    0.000123\n",
      "109                                 desc_ave    0.000123\n",
      "110      feature_outdoor_entertainment_space    0.000115\n",
      "111                     feature__dishwasher_    0.000103\n",
      "112                                desc_west    0.000089\n",
      "113                                 desc_200    0.000077\n",
      "114                        feature_furnished    0.000075\n",
      "115                  feature_private_terrace    0.000075\n",
      "116                     feature_cats_allowed    0.000063\n",
      "117                     feature_package_room    0.000063\n",
      "118                      feature_unit_washer    0.000043\n",
      "119                      feature_indoor_pool    0.000043\n",
      "120                          feature_simplex    0.000037\n",
      "121                       feature_dishwasher    0.000037\n",
      "122                           feature__dryer    0.000034\n",
      "123                            feature_multi    0.000029\n",
      "124                     feature_site_parking    0.000023\n",
      "125                  feature_wheelchair_ramp    0.000011\n",
      "126                 feature_pets_on_approval    0.000011\n",
      "127                        feature__pets_ok_    0.000009\n",
      "128                             feature_walk    0.000009\n",
      "129                             feature_flex    0.000006\n",
      "130                  feature__exposed_brick_    0.000006\n",
      "131                           feature_dryer_    0.000006\n",
      "132                  feature_gym_in_building    0.000003\n",
      "133                           feature_lounge    0.000003\n",
      "\n",
      "[134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(n_estimators = best_iteration,\n",
    "                              learning_rate=0.01,\n",
    "                              max_depth=int(BO_scores.to_dict()['max_depth'][0]),\n",
    "                              min_child_weight=BO_scores.to_dict()['min_child_weight'][0],\n",
    "                              colsample_bytree=BO_scores.to_dict()['colsample_bytree'][0],\n",
    "                              subsample=BO_scores.to_dict()['subsample'][0],\n",
    "                              gamma=BO_scores.to_dict()['gamma'][0],\n",
    "                              seed=1234,\n",
    "                              nthread=-1)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "feature_importance = pd.DataFrame(sorted(zip(full_vars,clf.feature_importances_)\n",
    "                          , key=lambda x: x[1], reverse = True),columns=['feature_name','importance']) \n",
    "\n",
    "print (feature_importance.query('importance>0'))\n",
    "\n",
    "\n",
    "preds = clf.predict_proba(test_x)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(r\"C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\sub_xgb_tuned_v8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   feature_fraction |   max_bins |   min_gain_to_split |   min_sum_hessian_in_leaf |   num_leaves | \n",
      "    1 | 00m46s | \u001b[35m  -0.52299\u001b[0m | \u001b[32m            0.9268\u001b[0m | \u001b[32m        2.0500\u001b[0m | \u001b[32m            0.6182\u001b[0m | \u001b[32m 1004.7449\u001b[0m | \u001b[32m             0.6965\u001b[0m | \u001b[32m                  13.2418\u001b[0m | \u001b[32m    111.0091\u001b[0m | \n",
      "    2 | 00m32s | \u001b[35m  -0.52186\u001b[0m | \u001b[32m            0.8552\u001b[0m | \u001b[32m        1.5082\u001b[0m | \u001b[32m            0.6259\u001b[0m | \u001b[32m  467.5715\u001b[0m | \u001b[32m             1.3656\u001b[0m | \u001b[32m                  11.4053\u001b[0m | \u001b[32m    319.3626\u001b[0m | \n",
      "    3 | 00m40s |   -0.52377 |             0.8099 |         2.0632 |             0.3571 |  1000.0635 |              0.7264 |                   72.6524 |     127.8515 | \n",
      "    4 | 00m28s |   -0.52192 |             0.7392 |         3.9515 |             0.6429 |   929.7169 |              1.5374 |                   75.2165 |     106.0693 | \n",
      "    5 | 00m25s | \u001b[35m  -0.52068\u001b[0m | \u001b[32m            0.7657\u001b[0m | \u001b[32m        3.1022\u001b[0m | \u001b[32m            0.2979\u001b[0m | \u001b[32m  350.3905\u001b[0m | \u001b[32m             1.2391\u001b[0m | \u001b[32m                  39.4883\u001b[0m | \u001b[32m    100.7528\u001b[0m | \n",
      "    6 | 00m26s |   -0.52072 |             0.9329 |         1.7632 |             0.4117 |   991.4283 |              1.6101 |                   92.3161 |     335.2921 | \n",
      "    7 | 00m52s |   -0.53146 |             0.7781 |         3.3462 |             0.3419 |   416.6094 |              0.0618 |                   18.4897 |     432.8270 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   feature_fraction |   max_bins |   min_gain_to_split |   min_sum_hessian_in_leaf |   num_leaves | \n",
      "    8 | 00m59s | \u001b[35m  -0.51880\u001b[0m | \u001b[32m            0.8676\u001b[0m | \u001b[32m        4.0327\u001b[0m | \u001b[32m            0.2665\u001b[0m | \u001b[32m  608.1980\u001b[0m | \u001b[32m             1.1023\u001b[0m | \u001b[32m                   1.3725\u001b[0m | \u001b[32m     20.0869\u001b[0m | \n",
      "    9 | 00m45s |   -0.51999 |             0.7038 |         1.2420 |             0.7544 |   128.2264 |              1.0748 |                   10.5095 |      16.0294 | \n",
      "   10 | 00m46s |   -0.52362 |             0.7884 |         3.9323 |             0.7220 |  1020.3678 |              1.3795 |                    2.5260 |     507.3534 | \n",
      "   11 | 00m41s |   -0.52140 |             0.8148 |         1.6582 |             0.2412 |   768.0080 |              1.9806 |                    1.3849 |     305.1307 | \n",
      "   12 | 01m03s |   -0.52188 |             0.8599 |         2.4061 |             0.6832 |   581.3352 |              0.5854 |                   99.6254 |     180.6145 | \n",
      "   13 | 00m52s |   -0.52024 |             0.7495 |         4.1246 |             0.5530 |   413.4371 |              0.5648 |                    2.5346 |      17.9573 | \n",
      "   14 | 01m22s |   -0.53087 |             0.8575 |         4.5013 |             0.7902 |   132.4791 |              0.4749 |                    1.6309 |     213.5609 | \n",
      "   15 | 00m45s |   -0.52188 |             0.9200 |         3.0564 |             0.5193 |   130.6730 |              1.9690 |                   99.6264 |      15.9542 | \n",
      "   16 | 00m49s |   -0.51952 |             0.7640 |         1.5336 |             0.4179 |   794.9197 |              0.6432 |                    5.0109 |      15.1758 | \n",
      "   17 | 00m43s |   -0.52162 |             0.8175 |         4.1127 |             0.7497 |   723.9088 |              1.9018 |                    1.5455 |     133.8158 | \n",
      "   18 | 00m53s |   -0.52172 |             0.8526 |         2.6520 |             0.5055 |   829.1938 |              1.5438 |                   98.2780 |     510.4533 | \n",
      "   19 | 01m05s |   -0.51992 |             0.9900 |         1.2330 |             0.7385 |   698.0969 |              0.2717 |                   98.5632 |      18.1419 | \n",
      "   20 | 00m48s |   -0.51966 |             0.9290 |         2.7858 |             0.2741 |   240.5032 |              0.3071 |                    2.5261 |      16.9275 | \n",
      "   21 | 01m16s |   -0.52696 |             0.8627 |         1.1237 |             0.7811 |   502.5995 |              0.4526 |                    1.2890 |     169.5770 | \n",
      "   22 | 00m37s |   -0.52203 |             0.9700 |         4.5746 |             0.5020 |   636.4245 |              1.4086 |                   99.2125 |     381.7228 | \n",
      "   23 | 00m48s |   -0.52228 |             0.9355 |         2.8173 |             0.3673 |   451.9927 |              1.2408 |                   98.8346 |      15.8697 | \n",
      "   24 | 01m02s |   -0.52305 |             0.9686 |         2.2785 |             0.3453 |  1015.5703 |              0.5655 |                   99.4093 |     510.5380 | \n",
      "   25 | 00m49s |   -0.52234 |             0.7908 |         4.2001 |             0.7126 |   824.9026 |              1.8247 |                   99.1019 |     295.6175 | \n",
      "   26 | 01m04s |   -0.51965 |             0.9533 |         4.6695 |             0.6384 |  1016.5788 |              0.9769 |                   91.4059 |      16.6123 | \n",
      "   27 | 00m49s |   -0.52216 |             0.7961 |         3.6218 |             0.3664 |   402.8349 |              1.8256 |                   98.6477 |     263.1906 | \n",
      "   28 | 00m46s |   -0.51962 |             0.7589 |         4.3668 |             0.3776 |   932.0549 |              1.5358 |                    6.5992 |      15.2901 | \n",
      "   29 | 00m41s |   -0.52387 |             0.9022 |         4.7715 |             0.4553 |   881.6919 |              1.8717 |                    3.6525 |     410.0786 | \n",
      "   30 | 00m51s |   -0.52170 |             0.8785 |         3.7706 |             0.6341 |   857.1441 |              1.0539 |                   99.2345 |      15.3943 | \n",
      "   31 | 00m51s | \u001b[35m  -0.51870\u001b[0m | \u001b[32m            0.9057\u001b[0m | \u001b[32m        3.2662\u001b[0m | \u001b[32m            0.3640\u001b[0m | \u001b[32m 1018.8381\u001b[0m | \u001b[32m             1.6832\u001b[0m | \u001b[32m                   4.3151\u001b[0m | \u001b[32m     17.2292\u001b[0m | \n",
      "   32 | 00m54s |   -0.51898 |             0.8154 |         2.4978 |             0.7460 |   708.2000 |              1.8595 |                    5.4240 |      15.8492 | \n",
      "   33 | 00m44s |   -0.52308 |             0.7045 |         1.9869 |             0.4846 |   288.5116 |              1.8561 |                   90.1052 |      15.2435 | \n",
      "   34 | 00m41s |   -0.52283 |             0.9847 |         4.6006 |             0.3552 |   594.6499 |              1.1528 |                   93.1280 |      15.1071 | \n",
      "   35 | 00m46s |   -0.52425 |             0.8500 |         1.2117 |             0.6425 |   705.9257 |              1.2075 |                    3.9519 |     506.8738 | \n",
      "   36 | 01m04s |   -0.52269 |             0.9222 |         1.3393 |             0.6926 |   690.2249 |              0.4188 |                   96.8038 |     243.2180 | \n",
      "   37 | 01m03s |   -0.52181 |             0.8279 |         1.3671 |             0.4919 |   526.8939 |              1.7718 |                   99.5480 |     334.6095 | \n",
      "   38 | 00m41s |   -0.52513 |             0.9613 |         4.0325 |             0.7035 |  1022.4455 |              1.0354 |                    2.9221 |     347.1959 | \n",
      "   39 | 01m39s |   -0.53718 |             0.7708 |         4.9569 |             0.7139 |   615.1513 |              0.1521 |                    1.2889 |     347.8240 | \n",
      "   40 | 00m51s |   -0.52167 |             0.7927 |         2.2609 |             0.3095 |   140.1180 |              1.9883 |                   99.0494 |     504.2366 | \n",
      "   41 | 00m45s |   -0.52149 |             0.8104 |         1.1928 |             0.2887 |   588.9610 |              1.7922 |                   98.0493 |     511.5395 | \n",
      "   42 | 01m30s |   -0.53929 |             0.7301 |         1.0444 |             0.4048 |   128.9243 |              0.5114 |                    2.9300 |     510.0856 | \n",
      "   43 | 00m48s |   -0.52182 |             0.9531 |         1.1556 |             0.6751 |   250.0100 |              1.5542 |                   99.9288 |     351.6463 | \n",
      "   44 | 00m44s |   -0.52183 |             0.9681 |         1.2498 |             0.3793 |   701.8753 |              1.7993 |                   97.0632 |     480.2154 | \n",
      "   45 | 00m56s |   -0.52153 |             0.8236 |         1.0295 |             0.4054 |   331.2627 |              1.8287 |                   98.4022 |     511.9339 | \n",
      "   46 | 00m42s |   -0.52289 |             0.9841 |         1.3410 |             0.4537 |   874.8724 |              1.6900 |                    1.1065 |     201.5384 | \n",
      "   47 | 00m57s |   -0.52177 |             0.8322 |         1.0441 |             0.6890 |   195.2005 |              1.4238 |                   97.7983 |     155.8390 | \n",
      "   48 | 00m57s |   -0.52178 |             0.7814 |         1.7549 |             0.6403 |   127.5637 |              1.5039 |                   99.3310 |     302.6243 | \n",
      "   49 | 00m48s |   -0.52120 |             0.8956 |         1.6853 |             0.3206 |   780.5416 |              1.8041 |                   94.6941 |     122.7011 | \n",
      "   50 | 00m56s |   -0.51964 |             0.8270 |         1.1402 |             0.3767 |   529.3822 |              0.3801 |                    3.4966 |      16.8993 | \n",
      "   51 | 00m52s |   -0.52223 |             0.8067 |         1.0091 |             0.4315 |   321.1777 |              1.7728 |                    2.9010 |     199.2444 | \n",
      "   52 | 00m43s |   -0.51962 |             0.9286 |         1.1650 |             0.2055 |   315.2111 |              1.1577 |                    1.5600 |      37.2480 | \n",
      "   53 | 01m02s |   -0.52178 |             0.7580 |         1.0282 |             0.7894 |   413.4690 |              1.6702 |                   99.2748 |     371.6872 | \n",
      "   54 | 00m54s |   -0.52224 |             0.7736 |         1.3511 |             0.5810 |   749.0304 |              1.7539 |                   99.5804 |     366.7482 | \n",
      "   55 | 00m54s |   -0.52172 |             0.7501 |         1.2737 |             0.7824 |   661.3823 |              1.9751 |                   29.0317 |      71.9425 | \n",
      "   56 | 00m52s |   -0.52011 |             0.7350 |         1.0485 |             0.4008 |   984.5802 |              1.3927 |                   49.1379 |      16.0077 | \n",
      "   57 | 01m05s |   -0.52197 |             0.8475 |         2.1402 |             0.6585 |   227.3743 |              1.5711 |                   99.7062 |     499.9896 | \n",
      "   58 | 00m49s |   -0.52299 |             0.7452 |         1.0719 |             0.4436 |   129.1549 |              1.1684 |                   99.3889 |     409.5276 | \n",
      "   59 | 00m49s |   -0.52140 |             0.9451 |         1.3749 |             0.2339 |   257.1962 |              1.8339 |                   99.8949 |     234.8043 | \n",
      "   60 | 00m55s |   -0.51986 |             0.8686 |         1.2588 |             0.4406 |   839.2232 |              1.7501 |                    2.8167 |      71.3932 | \n",
      "   61 | 00m53s |   -0.52319 |             0.8392 |         1.7206 |             0.4015 |   870.0283 |              1.3770 |                    2.7286 |     510.1206 | \n",
      "   62 | 00m53s |   -0.52123 |             0.9604 |         2.7951 |             0.5656 |  1020.3141 |              1.5043 |                   98.9759 |     250.1066 | \n",
      "   63 | 00m52s |   -0.52123 |             0.7162 |         2.2039 |             0.4197 |   251.3103 |              1.9031 |                    6.0460 |      86.4340 | \n",
      "   64 | 00m58s |   -0.51892 |             0.8711 |         1.1875 |             0.4210 |   633.1789 |              1.1594 |                    2.0502 |      17.0031 | \n",
      "   65 | 01m01s |   -0.52219 |             0.7736 |         1.5147 |             0.6215 |   482.6779 |              1.6001 |                   99.9609 |     506.1516 | \n",
      "   66 | 00m58s |   -0.52062 |             0.7493 |         4.8615 |             0.6626 |   758.7446 |              1.9780 |                   50.9969 |      19.9590 | \n",
      "   67 | 01m03s |   -0.52163 |             0.8109 |         1.6489 |             0.6833 |  1022.5096 |              1.4041 |                   95.8410 |     389.2521 | \n",
      "   68 | 00m57s |   -0.52276 |             0.7386 |         1.1444 |             0.5232 |   362.7919 |              1.4574 |                   99.1578 |     148.4820 | \n",
      "   69 | 01m07s |   -0.51955 |             0.8510 |         2.8713 |             0.5557 |   871.6275 |              1.8952 |                    1.6731 |      15.1320 | \n",
      "   70 | 01m04s |   -0.51904 |             0.8588 |         1.6121 |             0.7760 |   176.6392 |              1.5126 |                    2.1148 |      19.6389 | \n",
      "   71 | 00m53s |   -0.51938 |             0.9518 |         1.6458 |             0.2479 |   597.1912 |              1.3242 |                    4.6426 |      15.6615 | \n",
      "   72 | 01m12s |   -0.52226 |             0.7717 |         4.9876 |             0.6896 |   693.4452 |              1.2756 |                   99.9739 |     104.5670 | \n",
      "   73 | 00m57s |   -0.51989 |             0.9384 |         4.0768 |             0.2287 |   775.8436 |              1.5984 |                    1.3242 |      85.7405 | \n",
      "   74 | 01m05s |   -0.52157 |             0.8783 |         4.5657 |             0.4139 |   129.1949 |              1.5477 |                   99.7883 |     130.0252 | \n",
      "   75 | 01m03s | \u001b[35m  -0.51804\u001b[0m | \u001b[32m            0.8790\u001b[0m | \u001b[32m        4.9602\u001b[0m | \u001b[32m            0.2989\u001b[0m | \u001b[32m  166.6534\u001b[0m | \u001b[32m             0.7837\u001b[0m | \u001b[32m                  31.2747\u001b[0m | \u001b[32m     15.7327\u001b[0m | \n",
      "   76 | 00m54s |   -0.52040 |             0.9015 |         4.8816 |             0.4512 |   181.1028 |              1.9819 |                   44.6367 |      55.4867 | \n",
      "   77 | 01m10s |   -0.52525 |             0.9228 |         4.8685 |             0.2791 |   786.6539 |              0.1310 |                   35.3146 |     219.8630 | \n",
      "   78 | 01m08s |   -0.52266 |             0.7617 |         2.7380 |             0.6580 |   931.4215 |              1.9342 |                   99.5951 |     265.9336 | \n",
      "   79 | 00m58s |   -0.52107 |             0.9035 |         4.7405 |             0.4531 |   398.9231 |              1.7020 |                   14.0858 |     275.7642 | \n",
      "   80 | 01m09s |   -0.52030 |             0.8131 |         4.1883 |             0.7895 |   194.4560 |              1.5877 |                   26.6457 |      15.0092 | \n",
      "   81 | 01m03s |   -0.52365 |             0.9205 |         4.5533 |             0.2351 |   932.0487 |              0.6306 |                   99.7501 |     420.0672 | \n",
      "   82 | 01m17s |   -0.52293 |             0.9710 |         4.5428 |             0.3531 |   487.2416 |              0.3269 |                   99.9438 |     269.7077 | \n",
      "   83 | 01m50s |   -0.53522 |             0.7126 |         1.6048 |             0.7195 |   846.2416 |              0.4316 |                    2.1523 |     321.9864 | \n",
      "   84 | 01m12s |   -0.52207 |             0.7953 |         4.1128 |             0.5090 |   872.6616 |              1.9705 |                   98.4788 |     171.1974 | \n",
      "   85 | 01m08s |   -0.52025 |             0.8746 |         4.3565 |             0.2219 |   875.9618 |              1.9680 |                    1.5412 |     135.6700 | \n",
      "   86 | 01m04s |   -0.52199 |             0.7028 |         4.4181 |             0.3664 |   409.0533 |              1.8967 |                    1.8749 |     106.0706 | \n",
      "   87 | 01m09s |   -0.52200 |             0.7454 |         4.8193 |             0.5340 |   795.7900 |              1.8604 |                   13.9564 |     460.1147 | \n",
      "   88 | 01m10s |   -0.52084 |             0.9175 |         4.1198 |             0.2137 |   321.2674 |              1.9987 |                   99.4910 |     408.3359 | \n",
      "   89 | 01m08s |   -0.52133 |             0.8625 |         4.1642 |             0.3210 |   342.1991 |              1.7802 |                   98.9429 |     306.4979 | \n",
      "   90 | 01m00s |   -0.52205 |             0.9434 |         4.0149 |             0.2974 |   505.6265 |              1.9471 |                   99.5666 |     423.6727 | \n",
      "   91 | 01m17s |   -0.51952 |             0.8876 |         4.9187 |             0.2645 |   942.5959 |              0.5032 |                   99.8932 |      15.8439 | \n",
      "   92 | 01m11s |   -0.52014 |             0.7664 |         3.7929 |             0.4315 |   476.6708 |              1.8434 |                    1.0998 |      17.2752 | \n",
      "   93 | 01m09s |   -0.52292 |             0.7361 |         4.2760 |             0.2469 |   270.4173 |              1.0862 |                   99.6084 |     126.1509 | \n",
      "   94 | 01m11s |   -0.52099 |             0.9010 |         4.4120 |             0.2687 |   180.3506 |              1.4401 |                   99.9631 |     234.4746 | \n",
      "   95 | 01m14s |   -0.52080 |             0.9016 |         4.9568 |             0.4423 |   370.0735 |              1.9664 |                   42.7297 |      16.0758 | \n",
      "   96 | 01m13s |   -0.52170 |             0.8899 |         4.2649 |             0.2895 |   499.5882 |              1.8252 |                   97.0929 |     110.7265 | \n",
      "   97 | 01m12s |   -0.51946 |             0.8897 |         4.5514 |             0.3115 |   564.5073 |              1.4056 |                    5.3757 |      61.8556 | \n",
      "   98 | 01m28s |   -0.52323 |             0.8673 |         4.7825 |             0.4250 |   751.0365 |              0.7421 |                   98.8073 |     509.4220 | \n",
      "   99 | 01m14s |   -0.52223 |             0.9573 |         4.2380 |             0.2135 |   731.3339 |              1.9445 |                   99.0209 |      16.9068 | \n",
      "  100 | 01m21s |   -0.52027 |             0.8459 |         4.0704 |             0.4487 |   677.0283 |              1.5931 |                   52.9543 |      15.9976 | \n",
      "  101 | 01m13s |   -0.52005 |             0.7449 |         3.9851 |             0.3001 |   745.4669 |              1.9221 |                    1.3454 |      34.0155 | \n",
      "  102 | 01m07s |   -0.52243 |             0.9981 |         2.4344 |             0.3458 |   588.8456 |              1.6614 |                   99.6688 |     104.1249 | \n",
      "  103 | 01m21s |   -0.52010 |             0.9158 |         1.2044 |             0.4038 |   158.0060 |              1.4560 |                   38.3540 |      17.6852 | \n",
      "  104 | 01m15s |   -0.52071 |             0.9824 |         4.8358 |             0.3146 |   143.6228 |              0.5226 |                    1.7430 |      61.6680 | \n",
      "  105 | 01m10s |   -0.51981 |             0.9567 |         4.7065 |             0.2856 |   317.0677 |              1.4378 |                    1.3620 |     100.9059 | \n",
      "  106 | 01m32s |   -0.51938 |             0.8798 |         4.9444 |             0.3984 |   343.1799 |              0.2282 |                    4.7364 |      16.0269 | \n",
      "  107 | 01m24s |   -0.51996 |             0.7477 |         4.8562 |             0.7985 |   152.2514 |              1.9803 |                    7.0295 |      18.4935 | \n",
      "  108 | 01m30s |   -0.52134 |             0.9196 |         4.2876 |             0.3933 |   379.0228 |              0.0565 |                   91.7609 |      53.8986 | \n",
      "  109 | 01m24s |   -0.52268 |             0.9142 |         3.7127 |             0.3018 |   923.3064 |              0.0087 |                    1.1580 |      84.2836 | \n",
      "  110 | 01m24s |   -0.52427 |             0.8305 |         1.7090 |             0.2135 |   838.0940 |              0.0978 |                   50.2105 |     109.4076 | \n",
      "  111 | 01m19s |   -0.52381 |             0.9647 |         4.2532 |             0.5981 |  1017.7936 |              1.1213 |                    1.9770 |     229.5345 | \n",
      "  112 | 01m37s |   -0.52324 |             0.7976 |         4.8229 |             0.5333 |   821.2251 |              0.2274 |                   99.5865 |     431.3454 | \n",
      "  113 | 01m22s |   -0.52256 |             0.9887 |         4.3404 |             0.5515 |   749.7763 |              1.2329 |                   98.8509 |     293.3236 | \n",
      "  114 | 01m31s |   -0.52226 |             0.7483 |         4.2445 |             0.3789 |   914.2853 |              1.9350 |                   94.5008 |     510.3657 | \n",
      "  115 | 01m37s |   -0.52313 |             0.8437 |         4.8777 |             0.4040 |   204.7224 |              0.0149 |                   98.2287 |     299.6697 | \n",
      "  116 | 01m42s |   -0.52244 |             0.9193 |         3.1629 |             0.3748 |   127.3671 |              0.1926 |                   96.7554 |     200.4130 | \n",
      "  117 | 01m51s |   -0.52326 |             0.8748 |         4.8266 |             0.6147 |   261.6377 |              0.3628 |                   99.6699 |     428.6685 | \n",
      "  118 | 01m40s |   -0.53733 |             0.7498 |         1.0992 |             0.2006 |   956.4323 |              0.4016 |                    9.7646 |     460.1985 | \n",
      "  119 | 01m55s |   -0.52164 |             0.9935 |         4.7994 |             0.4018 |  1021.8226 |              0.2129 |                   99.6048 |     293.2362 | \n",
      "  120 | 01m34s |   -0.52113 |             0.9094 |         1.4145 |             0.4685 |  1022.2094 |              1.7308 |                   98.1288 |      67.9986 | \n",
      "  121 | 02m04s |   -0.53231 |             0.9709 |         4.5337 |             0.3552 |   309.6521 |              0.6711 |                    3.4689 |     303.3393 | \n",
      "  122 | 01m27s |   -0.52172 |             0.9722 |         4.3894 |             0.2678 |   391.1245 |              1.0423 |                   99.4819 |     507.4021 | \n",
      "  123 | 01m50s |   -0.52439 |             0.9687 |         4.9236 |             0.2762 |   384.1871 |              0.3539 |                   21.6836 |     190.3443 | \n",
      "  124 | 01m30s |   -0.52209 |             0.9120 |         3.5701 |             0.3069 |   724.0198 |              1.9222 |                    2.3676 |     242.5364 | \n",
      "  125 | 01m36s |   -0.52054 |             0.9466 |         4.7693 |             0.4661 |   618.8437 |              1.6634 |                    1.2459 |     109.2427 | \n",
      "  126 | 01m50s |   -0.52497 |             0.9955 |         4.8256 |             0.2559 |   243.5391 |              0.0078 |                   15.2953 |     157.0891 | \n",
      "  127 | 02m36s |   -0.53838 |             0.7482 |         4.5808 |             0.5664 |   790.7099 |              0.2612 |                    2.6188 |     511.3991 | \n",
      "  128 | 01m46s |   -0.52341 |             0.7413 |         4.2820 |             0.4068 |   755.9320 |              1.9556 |                    7.3708 |     404.9275 | \n",
      "  129 | 01m47s |   -0.52357 |             0.8318 |         3.1649 |             0.4818 |   578.3778 |              1.8976 |                    4.2139 |     510.3194 | \n",
      "  130 | 01m42s |   -0.51976 |             0.8728 |         1.5008 |             0.2035 |   838.2720 |              1.8929 |                   45.5150 |     433.9037 | \n",
      "  131 | 01m49s |   -0.52053 |             0.8600 |         4.8622 |             0.2250 |   645.1364 |              1.7640 |                   32.2449 |     493.1455 | \n",
      "  132 | 01m43s |   -0.52129 |             0.9870 |         1.3912 |             0.2122 |   607.7879 |              1.7925 |                   96.2718 |     440.3762 | \n",
      "  133 | 01m59s |   -0.52215 |             0.7798 |         1.8325 |             0.6054 |   752.6418 |              1.7112 |                   99.9177 |     195.5524 | \n",
      "  134 | 01m53s |   -0.52106 |             0.9180 |         1.4828 |             0.5354 |   658.4750 |              1.8760 |                   95.2242 |     511.9267 | \n",
      "  135 | 01m51s |   -0.52073 |             0.8906 |         2.9473 |             0.5137 |   802.8663 |              1.7629 |                    1.3689 |     144.4092 | \n",
      "  136 | 02m00s |   -0.52222 |             0.8154 |         2.1644 |             0.3228 |   828.8858 |              1.5016 |                    1.5468 |     433.0954 | \n",
      "  137 | 02m00s |   -0.52166 |             0.8659 |         4.9218 |             0.7376 |   598.8093 |              1.8616 |                   98.2283 |     252.1832 | \n",
      "  138 | 01m45s |   -0.52164 |             0.9873 |         4.2884 |             0.2808 |   857.9046 |              1.8951 |                   44.6217 |     475.8353 | \n",
      "  139 | 01m55s |   -0.52245 |             0.7804 |         3.3629 |             0.2017 |   894.8792 |              1.8313 |                   99.6607 |     359.9816 | \n",
      "  140 | 01m51s |   -0.52251 |             0.9943 |         2.1837 |             0.3397 |   440.4681 |              1.9504 |                   62.0667 |     320.1492 | \n",
      "  141 | 02m09s |   -0.52197 |             0.7783 |         1.1813 |             0.4352 |   958.1677 |              1.9382 |                   97.8332 |     187.7320 | \n",
      "  142 | 01m56s |   -0.52209 |             0.9869 |         4.2560 |             0.7277 |   914.7393 |              1.8093 |                   51.7312 |      16.3168 | \n",
      "  143 | 02m18s |   -0.52007 |             0.8210 |         4.7930 |             0.7793 |   374.0750 |              1.9297 |                    2.6140 |      52.9405 | \n",
      "  144 | 02m04s |   -0.52081 |             0.7386 |         3.3879 |             0.4078 |   976.2224 |              1.6054 |                    2.2367 |      15.7161 | \n",
      "  145 | 02m10s |   -0.52096 |             0.8133 |         1.4259 |             0.3164 |   760.1440 |              1.7020 |                   61.1970 |     446.4051 | \n",
      "  146 | 02m05s |   -0.52192 |             0.7025 |         2.9986 |             0.3323 |   508.7326 |              1.9396 |                    2.5160 |      65.8233 | \n",
      "  147 | 02m18s |   -0.52141 |             0.8420 |         2.4255 |             0.6906 |   974.0375 |              1.8841 |                   99.8469 |      40.8286 | \n",
      "  148 | 02m14s |   -0.52348 |             0.7905 |         4.5899 |             0.2881 |   190.0091 |              0.2806 |                   96.5065 |      63.4245 | \n",
      "  149 | 02m12s |   -0.52072 |             0.7735 |         4.5848 |             0.2763 |  1019.5819 |              1.8577 |                   40.8613 |      22.3356 | \n",
      "  150 | 02m12s |   -0.52146 |             0.9571 |         1.3522 |             0.3860 |   396.4021 |              1.1752 |                   99.6923 |     451.1101 | \n",
      "  151 | 02m27s |   -0.52191 |             0.8886 |         1.4125 |             0.3075 |   445.0939 |              1.9355 |                    3.4481 |     267.0364 | \n",
      "  152 | 02m08s |   -0.52173 |             0.9556 |         4.1916 |             0.2477 |   321.1372 |              1.5560 |                   98.4211 |     219.2254 | \n",
      "  153 | 02m40s |   -0.52134 |             0.8377 |         1.7330 |             0.2655 |   641.9492 |              1.9016 |                   99.3120 |     181.4492 | \n",
      "  154 | 02m22s |   -0.51915 |             0.9023 |         4.4339 |             0.6222 |   290.5846 |              1.8343 |                    1.8998 |      15.8863 | \n",
      "  155 | 02m25s |   -0.52009 |             0.9502 |         3.6798 |             0.3121 |   951.0819 |              1.7909 |                    2.5037 |     159.6725 | \n",
      "  156 | 02m10s |   -0.52002 |             0.9437 |         4.6872 |             0.4800 |   620.3449 |              1.2429 |                    1.0308 |      55.9012 | \n",
      "  157 | 02m26s |   -0.52159 |             0.8343 |         4.8537 |             0.2827 |   211.6620 |              0.4799 |                    1.0544 |      52.8311 | \n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(train_x, train_y)\n",
    "\n",
    "def lgb_evaluate(max_bins,\n",
    "                 num_leaves,\n",
    "                 min_sum_hessian_in_leaf,\n",
    "                 min_gain_to_split,\n",
    "                 feature_fraction,\n",
    "                 bagging_fraction,\n",
    "                 bagging_freq\n",
    "                 ):\n",
    "    params = dict()\n",
    "    params['objective'] = 'multiclass'\n",
    "    params['num_class'] = 3\n",
    "    params['learning_rate'] = 0.1\n",
    "    params['max_bins'] = int(max_bins)   \n",
    "    params['num_leaves'] = int(num_leaves)    \n",
    "    params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "    params['min_gain_to_split'] = int(min_gain_to_split)    \n",
    "    params['feature_fraction'] = feature_fraction\n",
    "    params['bagging_fraction'] = bagging_fraction\n",
    "    params['bagging_freq'] = int(bagging_freq)\n",
    "\n",
    "\n",
    "    cv_results = lgb.cv(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100000,\n",
    "                    nfold=5,\n",
    "                    early_stopping_rounds=100,\n",
    "                    metrics='multi_logloss',\n",
    "                    verbose_eval=False\n",
    "                   )\n",
    "\n",
    "    return -pd.DataFrame(cv_results)['multi_logloss-mean'].min()\n",
    "\n",
    "\n",
    "lgb_BO = BayesianOptimization(lgb_evaluate, \n",
    "                             {'max_bins': (127, 1023),\n",
    "                              'num_leaves': (15, 512),\n",
    "                              'min_sum_hessian_in_leaf': (1, 100),\n",
    "                              'min_gain_to_split': (0,2),\n",
    "                              'feature_fraction': (0.2, 0.8),\n",
    "                              'bagging_fraction': (0.7, 1),\n",
    "                              'bagging_freq': (1, 5)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "lgb_BO.maximize(init_points=7, n_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bagging_fraction</th>\n",
       "      <th>bagging_freq</th>\n",
       "      <th>feature_fraction</th>\n",
       "      <th>max_bins</th>\n",
       "      <th>min_gain_to_split</th>\n",
       "      <th>min_sum_hessian_in_leaf</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.878972</td>\n",
       "      <td>4.960192</td>\n",
       "      <td>0.298949</td>\n",
       "      <td>166.653383</td>\n",
       "      <td>0.783663</td>\n",
       "      <td>31.274728</td>\n",
       "      <td>15.732732</td>\n",
       "      <td>-0.518042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.905694</td>\n",
       "      <td>3.266224</td>\n",
       "      <td>0.364004</td>\n",
       "      <td>1018.838113</td>\n",
       "      <td>1.683158</td>\n",
       "      <td>4.315071</td>\n",
       "      <td>17.229199</td>\n",
       "      <td>-0.518704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.867569</td>\n",
       "      <td>4.032703</td>\n",
       "      <td>0.266497</td>\n",
       "      <td>608.197952</td>\n",
       "      <td>1.102316</td>\n",
       "      <td>1.372453</td>\n",
       "      <td>20.086930</td>\n",
       "      <td>-0.518795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.871087</td>\n",
       "      <td>1.187484</td>\n",
       "      <td>0.420960</td>\n",
       "      <td>633.178880</td>\n",
       "      <td>1.159392</td>\n",
       "      <td>2.050158</td>\n",
       "      <td>17.003101</td>\n",
       "      <td>-0.518920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.815361</td>\n",
       "      <td>2.497831</td>\n",
       "      <td>0.746002</td>\n",
       "      <td>708.200014</td>\n",
       "      <td>1.859532</td>\n",
       "      <td>5.423963</td>\n",
       "      <td>15.849160</td>\n",
       "      <td>-0.518983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bagging_fraction  bagging_freq  feature_fraction     max_bins  \\\n",
       "67          0.878972      4.960192          0.298949   166.653383   \n",
       "23          0.905694      3.266224          0.364004  1018.838113   \n",
       "0           0.867569      4.032703          0.266497   608.197952   \n",
       "56          0.871087      1.187484          0.420960   633.178880   \n",
       "24          0.815361      2.497831          0.746002   708.200014   \n",
       "\n",
       "    min_gain_to_split  min_sum_hessian_in_leaf  num_leaves     score  \n",
       "67           0.783663                31.274728   15.732732 -0.518042  \n",
       "23           1.683158                 4.315071   17.229199 -0.518704  \n",
       "0            1.102316                 1.372453   20.086930 -0.518795  \n",
       "56           1.159392                 2.050158   17.003101 -0.518920  \n",
       "24           1.859532                 5.423963   15.849160 -0.518983  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show tuning results\n",
    "lgb_BO_scores = pd.DataFrame(lgb_BO.res['all']['params'])\n",
    "lgb_BO_scores['score'] = pd.DataFrame(lgb_BO.res['all']['values'])\n",
    "lgb_BO_scores = lgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "lgb_BO_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "clfs = []\n",
    "lgb_clfs = []\n",
    "for p in lgb_BO_scores.head(10).iterrows():\n",
    "    clfs.append(lgb.LGBMClassifier(n_estimators = 10000,\n",
    "                                    learning_rate=0.01,\n",
    "                                    max_bin=int(p[1].to_dict()['max_bins']),\n",
    "                                    num_leaves=int(p[1].to_dict()['num_leaves']),\n",
    "                                    min_child_weight=int(p[1].to_dict()['min_sum_hessian_in_leaf']),\n",
    "                                    colsample_bytree=p[1].to_dict()['feature_fraction'],\n",
    "                                    subsample=p[1].to_dict()['bagging_fraction'],\n",
    "                                    subsample_freq=int(p[1].to_dict()['bagging_freq']),\n",
    "                                    min_split_gain=p[1].to_dict()['min_gain_to_split'],\n",
    "                                    seed=1234,\n",
    "                                    nthread=-1)\n",
    "                )\n",
    "\n",
    "# XGBoost    \n",
    "xgb_clfs = []\n",
    "for p in BO_scores.head(10).iterrows():\n",
    "    clfs.append(xgb.XGBClassifier(n_estimators = 10000,\n",
    "                                  learning_rate=0.01,\n",
    "                                  max_depth=int(p[1].to_dict()['max_depth']),\n",
    "                                  min_child_weight=int(p[1].to_dict()['min_child_weight']),\n",
    "                                  colsample_bytree=p[1].to_dict()['colsample_bytree'],\n",
    "                                  subsample=p[1].to_dict()['subsample'],\n",
    "                                  gamma=p[1].to_dict()['gamma'],\n",
    "                                  seed=1234,\n",
    "                                  nthread=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interest_levels = ['low', 'medium', 'high']\n",
    "\n",
    "tau = {\n",
    "    'low': 0.69195995, \n",
    "    'medium': 0.23108864,\n",
    "    'high': 0.07695141, \n",
    "}\n",
    "\n",
    "def correct(df):\n",
    "    y = df[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print(a)\n",
    "\n",
    "    def f(p):\n",
    "        for k in range(len(interest_levels)):\n",
    "            p[k] *= a[k]\n",
    "        return p / p.sum()\n",
    "\n",
    "    df_correct = df.copy()\n",
    "    df_correct[interest_levels] = df_correct[interest_levels].apply(f, axis=1)\n",
    "\n",
    "    y = df_correct[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print(a)\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99025408244876045, 1.0199119751777115, 1.0307702227597324]\n",
      "[0.9956691306468165, 1.0076912404379388, 1.0164188334778979]\n"
     ]
    }
   ],
   "source": [
    "sub_df2 = correct(sub_df)\n",
    "sub_df2.to_csv(r\"C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\sub_xgb_tuned_v8.1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def blend_model(clfs, train_x, train_y, test_x, num_class, blend_folds):\n",
    "    num_class = 3\n",
    "    blend_folds = 5\n",
    "\n",
    "    skf = model_selection.StratifiedKFold(n_splits=blend_folds,random_state=1234)\n",
    "    skf_ids = list(skf.split(train_x, train_y))\n",
    "\n",
    "\n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(clfs)*num_class))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(clfs)*num_class))\n",
    "    blend_scores = np.zeros ((blend_folds,len(clfs)))\n",
    "\n",
    "    print  (\"Start blending.\")\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print (\"Blending model\",j+1, clf)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], num_class))\n",
    "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            train_x_fold = train_x[train_ids]\n",
    "            train_y_fold = train_y[train_ids]\n",
    "            val_x_fold = train_x[val_ids]\n",
    "            val_y_fold = train_y[val_ids]\n",
    "            # Set n_estimators to a large number for early_stopping\n",
    "            clf.n_estimators = 10000000\n",
    "            \n",
    "            # Set evaluation metric\n",
    "            if type(clf).__name__=='LGBMClassifier':\n",
    "                metric = 'logloss' #LightGBM\n",
    "            else:\n",
    "                metric = 'mlogloss' #XGBoost            \n",
    "            clf.fit(train_x_fold, train_y_fold,\n",
    "                    eval_set=[(val_x_fold,val_y_fold)],\n",
    "                    eval_metric=metric,\n",
    "                    early_stopping_rounds=500,verbose=False)\n",
    "            val_y_predict_fold = clf.predict_proba(val_x_fold)\n",
    "            score = metrics.log_loss(val_y_fold,val_y_predict_fold)\n",
    "            print (\"LOGLOSS: \", score)\n",
    "            print (\"Best Iteration:\", clf.best_iteration)\n",
    "            blend_scores[i,j]=score\n",
    "            train_blend_x[val_ids, j*num_class:j*num_class+num_class] = val_y_predict_fold\n",
    "            test_blend_x_j = test_blend_x_j + clf.predict_proba(test_x)\n",
    "        test_blend_x[:,j*num_class:j*num_class+num_class] = test_blend_x_j/blend_folds\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(blend_scores[:,j])))\n",
    "    return train_blend_x, test_blend_x, blend_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def blend_model2(clfs, train_x, train_y, test_x, num_class, blend_folds):\n",
    "    num_class = 3\n",
    "    blend_folds = 5\n",
    "\n",
    "    skf = model_selection.StratifiedKFold(n_splits=blend_folds,random_state=1234)\n",
    "    skf_ids = list(skf.split(train_x, train_y))\n",
    "\n",
    "\n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(clfs)*num_class))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(clfs)*num_class))\n",
    "    blend_scores = np.zeros ((blend_folds,len(clfs)))\n",
    "\n",
    "    print  (\"Start blending.\")\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print (\"Blending model\",j+1, clf)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], num_class))\n",
    "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            train_x_fold = train_x[train_ids]\n",
    "            train_y_fold = train_y[train_ids]\n",
    "            val_x_fold = train_x[val_ids]\n",
    "            val_y_fold = train_y[val_ids]\n",
    "            clf.fit(train_x_fold, train_y_fold,\n",
    "                    eval_set=[(val_x_fold,val_y_fold)],\n",
    "                    eval_metric=metric,\n",
    "                    early_stopping_rounds=500,verbose=False)\n",
    "            val_y_predict_fold = clf.predict_proba(val_x_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb_clfs = []\n",
    "for p in BO_scores.head(4).iterrows():\n",
    "    xgb_clfs.append(xgb.XGBClassifier(n_estimators = 10000,\n",
    "                                  learning_rate=0.01,\n",
    "                                  max_depth=int(p[1].to_dict()['max_depth']),\n",
    "                                  min_child_weight=int(p[1].to_dict()['min_child_weight']),\n",
    "                                  colsample_bytree=p[1].to_dict()['colsample_bytree'],\n",
    "                                  subsample=p[1].to_dict()['subsample'],\n",
    "                                  gamma=p[1].to_dict()['gamma'],\n",
    "                                  seed=1234,\n",
    "                                  nthread=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start blending.\n",
      "Blending model 1 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.29894906310327574,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=166,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=31, min_split_gain=0.78366256935552192,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=15, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.87897197295970197, subsample_for_bin=50000,\n",
      "        subsample_freq=4, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 1 fold 1\n",
      "LOGLOSS:  0.540670778854\n",
      "Best Iteration: 6225\n",
      "Model 1 fold 2\n",
      "LOGLOSS:  0.49972019773\n",
      "Best Iteration: 7312\n",
      "Model 1 fold 3\n",
      "LOGLOSS:  0.500530066693\n",
      "Best Iteration: 6125\n",
      "Model 1 fold 4\n",
      "LOGLOSS:  0.506867485999\n",
      "Best Iteration: 5489\n",
      "Model 1 fold 5\n",
      "LOGLOSS:  0.532422423891\n",
      "Best Iteration: 4289\n",
      "Score for model 1 is 0.516042\n",
      "Blending model 2 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.36400419315092314,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01,\n",
      "        max_bin=1018, max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=4, min_split_gain=1.6831576784163769,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=17, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.90569381301191865, subsample_for_bin=50000,\n",
      "        subsample_freq=3, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 2 fold 1\n",
      "LOGLOSS:  0.543747726523\n",
      "Best Iteration: 7404\n",
      "Model 2 fold 2\n",
      "LOGLOSS:  0.504031279722\n",
      "Best Iteration: 4987\n",
      "Model 2 fold 3\n",
      "LOGLOSS:  0.5255927573\n",
      "Best Iteration: 4547\n",
      "Model 2 fold 4\n",
      "LOGLOSS:  0.508837991211\n",
      "Best Iteration: 6637\n",
      "Model 2 fold 5\n",
      "LOGLOSS:  0.535134139476\n",
      "Best Iteration: 6752\n",
      "Score for model 2 is 0.523469\n",
      "Blending model 3 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.26649739830506147,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=608,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=1, min_split_gain=1.1023158271654383,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=20, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.86756914008303831, subsample_for_bin=50000,\n",
      "        subsample_freq=4, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 3 fold 1\n",
      "LOGLOSS:  0.541381444081\n",
      "Best Iteration: 5469\n",
      "Model 3 fold 2\n",
      "LOGLOSS:  0.502276655389\n",
      "Best Iteration: 4306\n",
      "Model 3 fold 3\n",
      "LOGLOSS:  0.516664780003\n",
      "Best Iteration: 5978\n",
      "Model 3 fold 4\n",
      "LOGLOSS:  0.506663869334\n",
      "Best Iteration: 4728\n",
      "Model 3 fold 5\n",
      "LOGLOSS:  0.532061629858\n",
      "Best Iteration: 5742\n",
      "Score for model 3 is 0.519810\n",
      "Blending model 4 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.42096015865325243,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=633,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=2, min_split_gain=1.1593921617244909,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=17, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.8710866520643028, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 4 fold 1\n",
      "LOGLOSS:  0.540729786957\n",
      "Best Iteration: 4891\n",
      "Model 4 fold 2\n",
      "LOGLOSS:  0.501967916661\n",
      "Best Iteration: 3868\n",
      "Model 4 fold 3\n",
      "LOGLOSS:  0.513349300516\n",
      "Best Iteration: 7839\n",
      "Model 4 fold 4\n",
      "LOGLOSS:  0.508095748981\n",
      "Best Iteration: 4803\n",
      "Model 4 fold 5\n",
      "LOGLOSS:  0.531849587859\n",
      "Best Iteration: 5532\n",
      "Score for model 4 is 0.519198\n",
      "Blending model 5 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.74600165902250359,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=708,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=5, min_split_gain=1.8595324327969702,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=15, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.81536085642077083, subsample_for_bin=50000,\n",
      "        subsample_freq=2, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 5 fold 1\n",
      "LOGLOSS:  0.542663740784\n",
      "Best Iteration: 3932\n",
      "Model 5 fold 2\n",
      "LOGLOSS:  0.504475813604\n",
      "Best Iteration: 3999\n",
      "Model 5 fold 3\n",
      "LOGLOSS:  0.513028487984\n",
      "Best Iteration: 3669\n",
      "Model 5 fold 4\n",
      "LOGLOSS:  0.50971993998\n",
      "Best Iteration: 3847\n",
      "Model 5 fold 5\n",
      "LOGLOSS:  0.534953653581\n",
      "Best Iteration: 3876\n",
      "Score for model 5 is 0.520968\n",
      "Blending model 6 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.77600131435394282,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=176,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=2, min_split_gain=1.512594917516779,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=19, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.85881551759676289, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 6 fold 1\n",
      "LOGLOSS:  0.541591478518\n",
      "Best Iteration: 3639\n",
      "Model 6 fold 2\n",
      "LOGLOSS:  0.503470102758\n",
      "Best Iteration: 3181\n",
      "Model 6 fold 3\n",
      "LOGLOSS:  0.507946305517\n",
      "Best Iteration: 4201\n",
      "Model 6 fold 4\n",
      "LOGLOSS:  0.508864482598\n",
      "Best Iteration: 3786\n",
      "Model 6 fold 5\n",
      "LOGLOSS:  0.532971944769\n",
      "Best Iteration: 3513\n",
      "Score for model 6 is 0.518969\n",
      "Blending model 7 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.62219049061164378,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=290,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=1, min_split_gain=1.8343435826090373,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=15, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.90227384787796971, subsample_for_bin=50000,\n",
      "        subsample_freq=4, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 7 fold 1\n",
      "LOGLOSS:  0.555524970245\n",
      "Best Iteration: 3383\n",
      "Model 7 fold 2\n",
      "LOGLOSS:  0.509238076826\n",
      "Best Iteration: 4581\n",
      "Model 7 fold 3\n",
      "LOGLOSS:  0.516058053804\n",
      "Best Iteration: 3179\n",
      "Model 7 fold 4\n",
      "LOGLOSS:  0.517673135873\n",
      "Best Iteration: 4361\n",
      "Model 7 fold 5\n",
      "LOGLOSS:  0.542615264806\n",
      "Best Iteration: 3760\n",
      "Score for model 7 is 0.528222\n",
      "Blending model 8 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.39844599113696955,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=343,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=4, min_split_gain=0.22820432797373447,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=16, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.87976467213840592, subsample_for_bin=50000,\n",
      "        subsample_freq=4, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 8 fold 1\n",
      "LOGLOSS:  0.55457674799\n",
      "Best Iteration: 5442\n",
      "Model 8 fold 2\n",
      "LOGLOSS:  0.501531110609\n",
      "Best Iteration: 5690\n",
      "Model 8 fold 3\n",
      "LOGLOSS:  0.515877363523\n",
      "Best Iteration: 3740\n",
      "Model 8 fold 4\n",
      "LOGLOSS:  0.509493316542\n",
      "Best Iteration: 3938\n",
      "Model 8 fold 5\n",
      "LOGLOSS:  0.536963567809\n",
      "Best Iteration: 6364\n",
      "Score for model 8 is 0.523688\n",
      "Blending model 9 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.24786600910316242,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=597,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=4, min_split_gain=1.3242382347753621,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=15, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.95180050543376682, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 9 fold 1\n",
      "LOGLOSS:  0.542956013095\n",
      "Best Iteration: 4873\n",
      "Model 9 fold 2\n",
      "LOGLOSS:  0.504288011671\n",
      "Best Iteration: 4934\n",
      "Model 9 fold 3\n",
      "LOGLOSS:  0.529270672018\n",
      "Best Iteration: 5236\n",
      "Model 9 fold 4\n",
      "LOGLOSS:  0.508850970278\n",
      "Best Iteration: 5410\n",
      "Model 9 fold 5\n",
      "LOGLOSS:  0.533141953679\n",
      "Best Iteration: 5629\n",
      "Score for model 9 is 0.523702\n",
      "Blending model 10 LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.3115393929913583,\n",
      "        drop_rate=0.1, is_unbalance=False, learning_rate=0.01, max_bin=564,\n",
      "        max_depth=-1, max_drop=50, min_child_samples=10,\n",
      "        min_child_weight=5, min_split_gain=1.4056177052624863,\n",
      "        n_estimators=10000, nthread=-1, num_leaves=61, objective='binary',\n",
      "        reg_alpha=0, reg_lambda=0, scale_pos_weight=1, seed=1234,\n",
      "        sigmoid=1.0, silent=True, skip_drop=0.5,\n",
      "        subsample=0.88972194638223323, subsample_for_bin=50000,\n",
      "        subsample_freq=4, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 10 fold 1\n",
      "LOGLOSS:  0.541918884741\n",
      "Best Iteration: 3420\n",
      "Model 10 fold 2\n",
      "LOGLOSS:  0.502821858973\n",
      "Best Iteration: 3326\n",
      "Model 10 fold 3\n",
      "LOGLOSS:  0.528101652875\n",
      "Best Iteration: 4656\n",
      "Model 10 fold 4\n",
      "LOGLOSS:  0.507449657\n",
      "Best Iteration: 3664\n",
      "Model 10 fold 5\n",
      "LOGLOSS:  0.532273271154\n",
      "Best Iteration: 3354\n",
      "Score for model 10 is 0.522513\n",
      "Blending model 11 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.19099608403028878, gamma=1.9843921487267702,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=45, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99690241436222515)\n",
      "Model 11 fold 1\n",
      "LOGLOSS:  0.538845594898\n",
      "Best Iteration: 4123\n",
      "Model 11 fold 2\n",
      "LOGLOSS:  0.500102759218\n",
      "Best Iteration: 3532\n",
      "Model 11 fold 3\n",
      "LOGLOSS:  0.504729357693\n",
      "Best Iteration: 3635\n",
      "Model 11 fold 4\n",
      "LOGLOSS:  0.505696387955\n",
      "Best Iteration: 4169\n",
      "Model 11 fold 5\n",
      "LOGLOSS:  0.529227245969\n",
      "Best Iteration: 4553\n",
      "Score for model 11 is 0.515720\n",
      "Blending model 12 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.17069162723434997, gamma=1.9176248514788696,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=65, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.9913243818332802)\n",
      "Model 12 fold 1\n",
      "LOGLOSS:  0.539841474284\n",
      "Best Iteration: 5791\n",
      "Model 12 fold 2\n",
      "LOGLOSS:  0.501504234993\n",
      "Best Iteration: 5114\n",
      "Model 12 fold 3\n",
      "LOGLOSS:  0.500563440432\n",
      "Best Iteration: 5850\n",
      "Model 12 fold 4\n",
      "LOGLOSS:  0.505872531056\n",
      "Best Iteration: 5728\n",
      "Model 12 fold 5\n",
      "LOGLOSS:  0.530500547875\n",
      "Best Iteration: 5626\n",
      "Score for model 12 is 0.515656\n",
      "Blending model 13 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.18162763887352845, gamma=0.5318578627986017,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=54, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99296494453425344)\n",
      "Model 13 fold 1\n",
      "LOGLOSS:  0.539459174697\n",
      "Best Iteration: 4024\n",
      "Model 13 fold 2\n",
      "LOGLOSS:  0.501038465302\n",
      "Best Iteration: 3999\n",
      "Model 13 fold 3\n",
      "LOGLOSS:  0.503892132314\n",
      "Best Iteration: 4610\n",
      "Model 13 fold 4\n",
      "LOGLOSS:  0.506221264131\n",
      "Best Iteration: 4722\n",
      "Model 13 fold 5\n",
      "LOGLOSS:  0.53040405433\n",
      "Best Iteration: 4481\n",
      "Score for model 13 is 0.516203\n",
      "Blending model 14 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.17106961529261266, gamma=1.9711154951799947,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=48, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99862094681450975)\n",
      "Model 14 fold 1\n",
      "LOGLOSS:  0.53924714415\n",
      "Best Iteration: 4662\n",
      "Model 14 fold 2\n",
      "LOGLOSS:  0.500361213092\n",
      "Best Iteration: 3896\n",
      "Model 14 fold 3\n",
      "LOGLOSS:  0.504378320931\n",
      "Best Iteration: 3985\n",
      "Model 14 fold 4\n",
      "LOGLOSS:  0.506343835359\n",
      "Best Iteration: 4218\n",
      "Model 14 fold 5\n",
      "LOGLOSS:  0.529565955766\n",
      "Best Iteration: 5246\n",
      "Score for model 14 is 0.515979\n",
      "Blending model 15 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.40102551361700545, gamma=0.014075700624175846,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=49, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99928209929995626)\n",
      "Model 15 fold 1\n",
      "LOGLOSS:  0.540307181697\n",
      "Best Iteration: 2703\n",
      "Model 15 fold 2\n",
      "LOGLOSS:  0.500989647048\n",
      "Best Iteration: 2695\n",
      "Model 15 fold 3\n",
      "LOGLOSS:  0.503276008934\n",
      "Best Iteration: 2382\n",
      "Model 15 fold 4\n",
      "LOGLOSS:  0.506380846782\n",
      "Best Iteration: 3079\n",
      "Model 15 fold 5\n",
      "LOGLOSS:  0.530705829878\n",
      "Best Iteration: 3151\n",
      "Score for model 15 is 0.516332\n",
      "Blending model 16 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.14388329212685405, gamma=1.9637517077953304,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=31, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99901647066661114)\n",
      "Model 16 fold 1\n",
      "LOGLOSS:  0.540020393759\n",
      "Best Iteration: 4636\n",
      "Model 16 fold 2\n",
      "LOGLOSS:  0.501525671125\n",
      "Best Iteration: 4187\n",
      "Model 16 fold 3\n",
      "LOGLOSS:  0.51670899486\n",
      "Best Iteration: 2191\n",
      "Model 16 fold 4\n",
      "LOGLOSS:  0.507678925752\n",
      "Best Iteration: 4475\n",
      "Model 16 fold 5\n",
      "LOGLOSS:  0.529108115408\n",
      "Best Iteration: 6905\n",
      "Score for model 16 is 0.519008\n",
      "Blending model 17 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.18869463322968527, gamma=0.30474674875214469,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=51, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99460982075959037)\n",
      "Model 17 fold 1\n",
      "LOGLOSS:  0.539866553564\n",
      "Best Iteration: 3383\n",
      "Model 17 fold 2\n",
      "LOGLOSS:  0.500309204382\n",
      "Best Iteration: 3289\n",
      "Model 17 fold 3\n",
      "LOGLOSS:  0.5039740405\n",
      "Best Iteration: 3178\n",
      "Model 17 fold 4\n",
      "LOGLOSS:  0.506617784894\n",
      "Best Iteration: 4022\n",
      "Model 17 fold 5\n",
      "LOGLOSS:  0.529458085318\n",
      "Best Iteration: 4083\n",
      "Score for model 17 is 0.516045\n",
      "Blending model 18 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.2962859163211552, gamma=1.9724593158497519,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=70, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99146131497365309)\n",
      "Model 18 fold 1\n",
      "LOGLOSS:  0.540495838356\n",
      "Best Iteration: 5631\n",
      "Model 18 fold 2\n",
      "LOGLOSS:  0.502089112229\n",
      "Best Iteration: 5689\n",
      "Model 18 fold 3\n",
      "LOGLOSS:  0.498249517116\n",
      "Best Iteration: 7332\n",
      "Model 18 fold 4\n",
      "LOGLOSS:  0.506096698201\n",
      "Best Iteration: 6037\n",
      "Model 18 fold 5\n",
      "LOGLOSS:  0.531043817159\n",
      "Best Iteration: 5448\n",
      "Score for model 18 is 0.515595\n",
      "Blending model 19 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.14654875074125015, gamma=1.8768794866185456,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=48, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.99810875367472962)\n",
      "Model 19 fold 1\n",
      "LOGLOSS:  0.540065996124\n",
      "Best Iteration: 6311\n",
      "Model 19 fold 2\n",
      "LOGLOSS:  0.50242919781\n",
      "Best Iteration: 5886\n",
      "Model 19 fold 3\n",
      "LOGLOSS:  0.503766588206\n",
      "Best Iteration: 5912\n",
      "Model 19 fold 4\n",
      "LOGLOSS:  0.506815005063\n",
      "Best Iteration: 7002\n",
      "Model 19 fold 5\n",
      "LOGLOSS:  0.531334169571\n",
      "Best Iteration: 7765\n",
      "Score for model 19 is 0.516882\n",
      "Blending model 20 XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
      "       colsample_bytree=0.12819004019314678, gamma=1.991954837407633,\n",
      "       learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=45, missing=None, n_estimators=10000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1234, silent=True,\n",
      "       subsample=0.93635190887120034)\n",
      "Model 20 fold 1\n",
      "LOGLOSS:  0.540426454635\n",
      "Best Iteration: 5129\n",
      "Model 20 fold 2\n",
      "LOGLOSS:  0.501933241417\n",
      "Best Iteration: 5122\n",
      "Model 20 fold 3\n",
      "LOGLOSS:  0.505080122746\n",
      "Best Iteration: 5107\n",
      "Model 20 fold 4\n",
      "LOGLOSS:  0.507771145344\n",
      "Best Iteration: 6284\n",
      "Model 20 fold 5\n",
      "LOGLOSS:  0.531878431587\n",
      "Best Iteration: 4822\n",
      "Score for model 20 is 0.517418\n"
     ]
    }
   ],
   "source": [
    "train_blend_x, test_blend_x, blend_scores_x = blend_model(clfs, \n",
    "                                                        train_x, \n",
    "                                                        train_y, \n",
    "                                                        test_x, \n",
    "                                                        num_class=3, \n",
    "                                                        blend_folds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 60)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_blend_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = model_selection.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'log_loss',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  35 | elapsed:   53.3s remaining:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  35 | elapsed:  1.2min remaining:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  35 | elapsed:  1.8min remaining:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.513\n",
      "Best parameters set: {'hidden_layer_sizes': 100}\n",
      "Scores: [mean: -0.51687, std: 0.01564, params: {'hidden_layer_sizes': 10}, mean: -0.51616, std: 0.01666, params: {'hidden_layer_sizes': 50}, mean: -0.51347, std: 0.01586, params: {'hidden_layer_sizes': 100}, mean: -0.51491, std: 0.01519, params: {'hidden_layer_sizes': 200}, mean: -0.51649, std: 0.01479, params: {'hidden_layer_sizes': 300}, mean: -0.51547, std: 0.01578, params: {'hidden_layer_sizes': 500}, mean: -0.51374, std: 0.01674, params: {'hidden_layer_sizes': 800}]\n",
      "best subsample: {'hidden_layer_sizes': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"hidden_layer_sizes\":[10,50,100,200,300, 500,800]\n",
    "              }\n",
    "model = search_model(train_blend_x\n",
    "                                         , train_y\n",
    "                                         , MLPClassifier()\n",
    "                                         , param_grid\n",
    "                                         , n_jobs=-1\n",
    "                                         , cv=5\n",
    "                                         , refit=True)   \n",
    "\n",
    "print (\"best subsample:\", model.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict_proba(test_blend_x)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(r\"C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions/sub_all_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98778984534963388, 1.0135358429239887, 1.0764807875146518]\n",
      "[0.99342071698107903, 1.010199501846899, 1.0301138452209655]\n"
     ]
    }
   ],
   "source": [
    "sub_df2 = correct(sub_df)\n",
    "sub_df2.to_csv(r\"C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\all_corrected_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "PandasError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPandasError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-8d5c55141a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_blend_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Anaconda2\\envs\\snakes\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    343\u001b[0m                                          copy=False)\n\u001b[1;32m    344\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mPandasError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPandasError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "pdtrain_blend_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_blend_x = sparse.hstack([train_x, train_blend_x]).tocsr()\n",
    "test_blend_x = sparse.hstack([test_x, test_blend_x]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-41c654c6d4a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_blend_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Anaconda2\\envs\\snakes\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    171\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    172\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown algorithm %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anaconda2\\envs\\snakes\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 364\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[1;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anaconda2\\envs\\snakes\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LU'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'QR'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anaconda2\\envs\\snakes\\lib\\site-packages\\scipy\\linalg\\decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \"\"\"\n\u001b[1;32m    177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anaconda2\\envs\\snakes\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1215\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m   1216\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = 120)\n",
    "svd.fit_transform(train_blend_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[745]\ttrain-mlogloss:0.430923+0.00170042\ttest-mlogloss:0.513554+0.00637356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "params['objective'] = 'multi:softprob'\n",
    "params['num_class'] = 3\n",
    "params['eta'] = 0.01\n",
    "params['max_depth'] = 7\n",
    "params['min_child_weight'] = 15\n",
    "params['subsample'] = 1\n",
    "params['colsample_bytree'] = 0.33\n",
    "params['gamma'] = 0.35\n",
    "params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(params, xgb.DMatrix(train_blend_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "               num_boost_round=1000000, nfold=5,\n",
    "       metrics={'mlogloss'},\n",
    "       seed=1234,\n",
    "       callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "best_xgb_score = cv_results['test-mlogloss-mean'].min()\n",
    "best_xgb_iteration = len(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(learning_rate = 0.01\n",
    "                  , n_estimators =best_xgb_iteration\n",
    "                  , max_depth = 7\n",
    "                  , min_child_weight = 15\n",
    "                  , subsample = 1\n",
    "                  , colsample_bytree = 0.33\n",
    "                  , gamma = 0.35\n",
    "                  , seed = 1234\n",
    "                  , nthread = -1\n",
    "                  )\n",
    "\n",
    "clf.fit(train_blend_x, train_y)\n",
    "\n",
    "preds = clf.predict_proba(test_blend_x)\n",
    "sub_df3 = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df3[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df3.to_csv(r\"C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0020531527463894, 1.0092896711547257, 0.95612504248874741]\n",
      "[1.0018611752049671, 0.99838347046353215, 0.98846793058918203]\n"
     ]
    }
   ],
   "source": [
    "sub_df4 = correct(sub_df3)\n",
    "sub_df4.to_csv(r\"C:\\Users\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\final_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
